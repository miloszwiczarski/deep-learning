{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "<font size=\"5\">\n",
    "\n",
    "Laboratorium z przedmiotu: \\\n",
    "**Głębokie uczenie i analiza obrazów**\n",
    "\n",
    "Ćwiczenie 1: \\\n",
    "**Regresja logistyczna i inne klasyczne algorytmy klasyfikacji**\n",
    "\n",
    "</font>\n",
    "\n",
    "\\\n",
    "Marta Szarmach \\\n",
    "Zakład Telekomunikacji Morskiej \\\n",
    "Wydział Elektryczny \\\n",
    "Uniwersytet Morski w Gdyni\n",
    "\n",
    "07.2023\n",
    "</div>\n",
    "\n",
    "\n",
    "# 1. Wprowadzenie\n",
    "\n",
    "**Uczenie maszynowe** jest to zbiór algorytmów, które odnajdują zależności ukryte w danych, potrafią te zależności modelować (tj. opisywać za pomocą matematycznych struktur) bez bycia jawnie zaprogramowanym przez człowieka i doskonalą swoje działanie (tj. potrafią dobierać lepsze parametry opisujące modele) dzięki dostarczeniu do nich nowych danych (uczenie maszynowe jest jedynie częścią szerszego zagadnienia, jakim jest **sztuczna inteligencja**, tj. dziedzina nauki z pogranicza informatyki, matematyki, kongwinistyki i neurologii, zajmująca się tworzeniem maszyn/oprogramowania ,,udających'' ludzką inteligencję, tj. potrafiącego analizować dostarczone doń dane, wyciągać na ich podstawie wnioski i podejmować decyzje).  Innymi słowy, w przeciwieństwie do klasycznego programowania, nie tworzy się gotowych reguł, lecz algorytm sam, na podstawie dostarczonych danych i spodziewanych odpowiedzi na nie, określa reguły, tzn. tworzy **model**, który stara się odzwierciedlić strukturę danych i sposób wyznaczania tychże odpowiedzi. Model najczęściej opisany jest za pomocą fukcji (hipotezy) zależnej od **parametrów** $\\theta$, które są pewnymi wartościami liczbowymi ulegającymi zmianie w ramach treningu (nie należy mylić parametrów z **hiperparametrami**, które również opisują w pewnym sensie model, lecz nie są wyznaczane podczas treningu, a wręcz przeciwnie, muszą być podane wcześniej).\n",
    "\n",
    "Aby stworzyć i wytrenować model, najczęściej określa się pewną funkcję (nazywaną **funkcją kosztu** $J(\\theta)$), która jest w stanie policzyć, jak bardzo model myli się podczas dokonywania predykcji. **Trening** polega na optymalizacji funkcji kosztu, a dokładniej mówiąc, iteracyjnej aktualizacji parametrów modelu, aż koszt na danych treningowych staje się (najczęściej) minimalny - wówczas uważa się, że model możliwie najlepiej odzwierciedla te dane. Jedną z metod optymalizacji jest **metoda gradientu prostego**, której działanie opiera się na poszukiwaniu lokalnego minimum poprzez wyznaczanie gradientu $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ (kierunku najszybszej zmiany) funkcji kosztu i aktualizacji parametrów zgodnie z tym gradientem. \n",
    "\n",
    "\\\n",
    "Uczenie maszynowe można podzielić na dwie podstawowe grupy: \n",
    "* **uczenie nadzorowane**, w którym każdy rekord danych dostarczony do algorytmu posiada **etykietę** zawierającą pożądaną odpowiedź algorytmu na ten konkretny rekord. Algorytmy uczenia nadzorowanego potrafią rozwiązywać takie problemy jak:\n",
    "    * **regresja** - przypisanie do rekordu danych pewnej dowolnej liczby,\n",
    "    * **klasyfikacja** -  przypisanie do rekordu danych liczby (z przedziału dyskretnego), symbolizującej jego przynależność do pewnej klasy,\n",
    "* **uczenie nienadzorowane** - dane nie posiadają predefiniowanych etykiet, algorytmy muszą same znaleźć strukturę w danych. Rozwiązują takie problemy, jak m.in.:\n",
    "    * **grupowanie** - podział zebranych danych na grupy, tak, aby dane z jednej grupy były bardziej podobne do siebie niż do danych z innych grup,\n",
    "    * **wykrywanie anomalii** - odnalezienie w zbiorze danych tych rekordów, które w pewien sposób odróżniają się od reszty.\n",
    "\n",
    "W niniejszym ćwiczeniu zajmować się będziemy jedynie zagadnieniem klasyfikacji. Niektórymi algorytmami uczenia maszynowego realizującymi klasyfikację są:\n",
    "* **Maszyna wektrów wspierających** (ang. *Support Vector Machine*, **SVM**) - algorytm, który dokonuje klasyfikacji danych poprzez utworzenie (z pomocą dodatkowych funkcji, tzw. kerneli) dodatkowego wymiaru i hiperpłaszczyzny, która oddziela na tym wymiarze dane z różnych klas z maksymalnym możliwym marginesem,\n",
    "* **Drzewo decyzyjne** (ang. *Decision Tree*) - zbiór hierarchicznie następujących po sobie instrukcji warunkowych, których ostatnia warstwa decyduje o wyniku predykcji,\n",
    "* *k* **najbliższych sąsiadów** (ang. *k Nearest Neighbours*, *k*-NN) - to, jaka zostanie podjęta decyzja dotycząca badanego rekordu, zależy od etykiet $k$ innych rekordów najbliższych temu rekordowi,\n",
    "* a także **regresja logistyczna** - która to będzie głównym zagadnieniem niniejszego ćwiczenia. W toku działania tegoż algorytmu, dokonuje się dopasowania pewnej funkcji, wiążącej parametry $\\theta$ i dane X, której zadaniem jest oszacowanie prawdopodobieństwa, z jakim określony rekord danych należy do pewnej klasy.\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:828/1*PQ8tdohapfm-YHlrRIRuOA.gif' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cel ćwiczenia\n",
    "\n",
    "**Celem niniejszego ćwiczenia** jest zapoznanie się z działaniem klasycznych algorytmów uczenia maszynowego realizujących zagadnienia klasyfikacji poprzez:\n",
    "* implementacji ,,od zera'' algorytmu regresji logistycznej (łącznie z optymalizacją funkcji kosztu metodą gradientu prostego),\n",
    "* użycie gotowych klas z biblioteki Scikit-learn z zaimplementowanymi gotowymi klasyfikatorami (regresją logistyczną, drzewem decyzyjnym i *k*-NN) i porównanie otrzymanych wyników.\n",
    "\n",
    "\n",
    "# 3. Stanowisko laboratoryjne\n",
    "\n",
    "Do wykonania niniejszego ćwiczenia niezbędne jest stanowisko laboratoryjne, składające się z komputera klasy PC z zainstalowanym oprogramowaniem:\n",
    "* językiem programowania Python (w wersji 3.8),\n",
    "* IDE obsługującym pliki Jupyter Notebook (np. Visual Studio Code z rozszerzeniem ipykernel).\n",
    "\n",
    "\n",
    "# 4. Przebieg ćwiczenia\n",
    "## 4.1. Implementacja algorytmu regresji logistycznej ,,od zera''\n",
    "\n",
    "### Inicjalizacja: import niezbędnych elementów\n",
    "\n",
    "Na początku wykonaj poniższy fragment kodu, aby zaimportować biblioteki niezbędne do wykonania poniższego ćwiczenia:\n",
    "* **Scikit-learn** - biblioteka zawierająca gotowe implementacje wielu algorytmów klasycznego uczenia maszynowego, a także zbiory danych czy metryki. Tutaj skorzystamy ze zbioru danych iris - `datasets.load_iris`.\n",
    "* **NumPy** - biblioteka umożliwiająca wykonywanie wysoko zoptymalizowanych obliczeń matematycznych na objektach typu *numpy array* (wielowymiarowych tablic).\n",
    "* **Matplotlib** - biblioteka wspomagająca wizualizację pracy czy analizę danych poprzez wyświetlanie wykresów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "! python -m pip install numpy==1.22.3\n",
    "! python -m pip install scikit-learn==0.24.2\n",
    "! python -m pip install matplotlib==3.4.2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po zaimportowaniu niezbędnych bibliotek, załadujmy gotowy zbiór danych Iris (zawierający dane o wymiarach różnych rodzajów irysów), pochodzący z repozytorium UCI (więcej informacji o tym zbiorze danych możesz uzyskać [TUTAJ](https://archive.ics.uci.edu/dataset/53/iris)). Dane zapiszmy pod zmienną X, a odpowiadające im etykiety - y.\n",
    "\n",
    "Ponadto, dla zobrazowania danych, które będziemy używać, wyświetlmy rozmiary tablic X i y, a także 5 pierszwych rekordów/etykiet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych wejściowych: (100, 4)\n",
      "Przykładowe dane wejściowe: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "Wymiary etykiet: (100,)\n",
      "Przykładowe etykiety: \n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[0:100,:]\n",
    "y = iris.target[0:100]\n",
    "print(\"Wymiary danych wejściowych: \" + str(X.shape))\n",
    "print(\"Przykładowe dane wejściowe: \")\n",
    "print(X[0:5,:])\n",
    "print(\"Wymiary etykiet: \" + str(y.shape))\n",
    "print(\"Przykładowe etykiety: \")\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "iris2 = load_iris()\n",
    "print(iris2['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przyjrzyjmy się wyświetlonym informacjom na temat wczytanych danych. Wczytujemy 100 rekordów o 4 cechach (wymiary tablicy X to 100x4). Etykiety zapisane są w formie 1-wymiarowego wektora y (o wymiarze 100) - z dokumentacji możemy się dowiedzieć, że mogą one przyjmować jedną z 3 wartości: 0, 1 lub 2, symbolizujących przynależność rekordu do jednej z 3 klas (każda klasa oznacza inny typ irysa), lecz w naszym przypadku, aby skupić się jedynie na zagadnieniu binarnej klasyfikacji, podczas ładowania danych odrzucamy ostatnich 50 rekordów należących do 3 klasy, więc operujemy jedynie na 2 klasach.\n",
    "\n",
    "<font size=\"2\">Informacje o wymiarach otrzymanych tablic będą nam bardzo potrzebne na późniejszym etapie, podczas pracy na macierzach przy implementacji procesu terningu i predykcji naszego algorytmu.</font>\n",
    "\n",
    "\n",
    "### Przygotowanie danych\n",
    "\n",
    "Aby móc w pełni korzystać z tych danych, musimy je jednak nieco przekształcić. W tym celu wykonamy dwie operacje:\n",
    "* dokonamy **standaryzacji** danych, tj. przekształcimy je tak, aby bez zmiany ich struktury, każda z cech posiadała średnią o wartości 0 i wariancję o wartości 1 - poprawi to działanie algorytmów uczenia maszynowego, zwłaszcza podczas pracy nad danymi o szerokiej dynamice (różnych rzędach wielkości) czy różnych jednostkach,\n",
    "* podzielimy cały dostępny zbiór danych na 2 zestawy: \n",
    "    * **treningowy** - na podstawie którego model zostanie wytrenowany (to pod te dane zostaną dopasowane parametry naszego modelu),\n",
    "    * **testowy** - który posłuży nam do określenia, jak dobrze działa nasz model na danych, których wcześniej model nie widział.\n",
    "\n",
    "<font size=\"2\">W tym konkretnym przypadku nie będziemy jeszcze wydzielać trzeciego z zazwyczaj tworzonych zestawów danych, zestawu **walidacyjnego**, na podstawie którego dopasowuje się hiperparametry modelu zanim przejdzie się do jego właściwego uczenia - nie ma takiej potrzeby, gdyż nie będziemy w ramach tego ćwiczenia zajmować się dopasowaniem żadnych hiperparametrów.</font>\n",
    "\n",
    "Zacznijmy od napisania funkcji `standarize_data` służącej do normalizacji danych. Zgodnie z poniższym wzorem, zaimplementuj funkcję, która przyjmuje jako argument wejściowe (tablicę X), oblicza średnią $\\mu_n$ i odchylenie standardowe $\\sigma_n$ każdej z cech $x_n$, a następnie odejmuje od nich wyliczone średnie i dzieli je przez odchylenia stardardowe: \n",
    "\\begin{equation*}\n",
    "\tx_n = \\frac{x_n - \\mu_n}{\\sigma_n}\n",
    "\\end{equation*}\n",
    "po czym zwraca tak znormalizowane dane jako tablicę Xnorm.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `standarize_data`:\n",
    "* Zapoznaj się z dokumentacją dwóch funkcji z biblioteki NumPy: `np.mean` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.mean.html)) oraz `np.std` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.std.html)). Zwróć uwagę na parametr `axis`.\n",
    "* Zastosuj mechanizm broadcastigu występujący podczas operacji na tablicach z użyciem NumPy (więcej o tym możesz przeczytać [TUTAJ](https://numpy.org/doc/stable/user/basics.broadcasting.html)).\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63848179, 0.47633916, 1.44228257, 0.56232019])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(X, axis=0)\n",
    "np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_data(X):\n",
    "    \"\"\"Funkcja realizująca standaryzację danych: przyrównanie średniej każdej z cech do 0, \n",
    "    a wariancji do 1.\n",
    "\n",
    "    Argument: \n",
    "        - X - nieprzekształcone dane (numpy array, shape = (num_samples, num_features) ). \\n\n",
    "\n",
    "    Zwraca:\n",
    "        - Xnorm - znormalizowane dane (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "        - mu - wektor ze średnimi każdej z cech (numpy array, shape = (num_features,) ), \\n\n",
    "        - sigma - wektor z odchyleniami standardowymi każdej z cech (numpy array, shape = (num_features,) ).\"\"\"\n",
    "\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz mu - średnią każdej z cech (pamiętaj, że cechy przechowywane są w każdej kolummnie tablicy X)\n",
    "    mu = np.average(X, axis=0)\n",
    "    # Oblicz sigma - odchylenie standardowe każdej z cech\n",
    "    sigma = np.std(X, axis=0)\n",
    "    # Oblicz Xnorm - odejmij średnią od każdej z cech i podziel ją przez jej odchylenie standardowe \n",
    "    # (możesz to zrobić w 1 linijce kodu)\n",
    "    Xnorm = (np.array(X) - mu) / sigma\n",
    "    # -------------------------------\n",
    "    return Xnorm, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -8.04974023e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  6.31902691e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.67741667e+00, -4.17769553e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [-1.11201292e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.83403820e+00, -2.07835104e-01, -1.22098127e+00,\n",
       "         -1.21994552e+00],\n",
       "        [ 5.15284858e-01,  1.89150938e+00, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 3.58663321e-01,  2.73124718e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.11201292e-01,  1.68157493e+00, -1.08231219e+00,\n",
       "         -6.86441647e-01],\n",
       "        [-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [ 3.58663321e-01,  1.47164049e+00, -8.04974023e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -9.43643106e-01,\n",
       "         -8.64276271e-01],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -8.04974023e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.36417359e+00,  1.05177159e+00, -1.29031581e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  4.21968242e-01, -8.04974023e-01,\n",
       "         -5.08607024e-01],\n",
       "        [-1.05093052e+00,  6.31902691e-01, -6.66304941e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01, -2.07835104e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  6.31902691e-01, -8.74308565e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  8.41837140e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-4.24444366e-01,  6.31902691e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-1.20755205e+00,  2.12033793e-01, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.05093052e+00,  2.09934449e-03, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.11201292e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-4.24444366e-01,  2.10144383e+00, -9.43643106e-01,\n",
       "         -1.21994552e+00],\n",
       "        [ 4.54202458e-02,  2.31137828e+00, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  2.12033793e-01, -1.15164673e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 4.54202458e-02,  8.41837140e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-8.94308978e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "         -1.21994552e+00],\n",
       "        [-1.67741667e+00, -2.07835104e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-5.81065904e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.52079513e+00, -1.67737625e+00, -1.08231219e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-1.67741667e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  8.41837140e-01, -8.74308565e-01,\n",
       "         -3.30772400e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -6.66304941e-01,\n",
       "         -6.86441647e-01],\n",
       "        [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "         -8.64276271e-01],\n",
       "        [-5.81065904e-01,  1.47164049e+00, -8.74308565e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-1.36417359e+00,  2.12033793e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [-2.67822829e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "         -1.04211089e+00],\n",
       "        [-7.37687441e-01,  4.21968242e-01, -1.01297765e+00,\n",
       "         -1.04211089e+00],\n",
       "        [ 2.39474331e+00,  2.12033793e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.45501408e+00,  2.12033793e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.23812177e+00,  2.09934449e-03,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 4.54202458e-02, -1.67737625e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.61163562e+00, -6.27704002e-01,  1.20572767e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  1.13639313e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00,  4.21968242e-01,  1.27506221e+00,\n",
       "          1.44757384e+00],\n",
       "        [-8.94308978e-01, -1.46744180e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.76825716e+00, -4.17769553e-01,  1.20572767e+00,\n",
       "          9.14069966e-01],\n",
       "        [-4.24444366e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          1.09190459e+00],\n",
       "        [-7.37687441e-01, -2.30717959e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 6.71906395e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01, -1.88731069e+00,  7.89720424e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 9.85149470e-01, -4.17769553e-01,  1.27506221e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -4.17769553e-01,  5.12382260e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  8.59054966e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 1.14177101e+00, -1.88731069e+00,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 2.02041783e-01, -1.25750735e+00,  7.20385883e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 6.71906395e-01,  2.12033793e-01,  1.34439675e+00,\n",
       "          1.80324308e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.29839254e+00, -1.25750735e+00,  1.41373130e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 9.85149470e-01, -6.27704002e-01,  1.27506221e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 1.45501408e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.76825716e+00, -2.07835104e-01,  1.06705859e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 2.08150023e+00, -6.27704002e-01,  1.34439675e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 1.92487869e+00, -2.07835104e-01,  1.48306584e+00,\n",
       "          1.62540846e+00],\n",
       "        [ 8.28527933e-01, -4.17769553e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 3.58663321e-01, -1.04757290e+00,  4.43047718e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  6.51051342e-01,\n",
       "          5.58400718e-01],\n",
       "        [ 4.54202458e-02, -1.46744180e+00,  5.81716801e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 5.15284858e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 8.28527933e-01, -8.37638451e-01,  1.55240038e+00,\n",
       "          1.44757384e+00],\n",
       "        [-1.11201292e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 8.28527933e-01,  6.31902691e-01,  1.13639313e+00,\n",
       "          1.44757384e+00],\n",
       "        [ 1.92487869e+00,  2.09934449e-03,  1.27506221e+00,\n",
       "          1.26973921e+00],\n",
       "        [ 1.29839254e+00, -1.67737625e+00,  1.06705859e+00,\n",
       "          9.14069966e-01],\n",
       "        [ 2.02041783e-01, -2.07835104e-01,  8.59054966e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.25750735e+00,  7.89720424e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 4.54202458e-02, -1.04757290e+00,  1.06705859e+00,\n",
       "          7.36235342e-01],\n",
       "        [ 9.85149470e-01, -2.07835104e-01,  1.20572767e+00,\n",
       "          1.09190459e+00],\n",
       "        [ 5.15284858e-01, -1.04757290e+00,  7.89720424e-01,\n",
       "          7.36235342e-01],\n",
       "        [-7.37687441e-01, -1.67737625e+00,  3.04378636e-01,\n",
       "          3.80566095e-01],\n",
       "        [ 2.02041783e-01, -8.37638451e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 3.58663321e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "          7.36235342e-01],\n",
       "        [ 3.58663321e-01, -4.17769553e-01,  9.28389507e-01,\n",
       "          9.14069966e-01],\n",
       "        [ 1.14177101e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "          9.14069966e-01],\n",
       "        [-5.81065904e-01, -1.25750735e+00,  9.63750123e-02,\n",
       "          5.58400718e-01],\n",
       "        [ 3.58663321e-01, -6.27704002e-01,  8.59054966e-01,\n",
       "          9.14069966e-01]]),\n",
       " array([5.471, 3.099, 2.861, 0.786]),\n",
       " array([0.63848179, 0.47633916, 1.44228257, 0.56232019]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standarize_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz przejdźmy do utworzenia funkcji `split_data`, która ma za zadanie podzielić dostarczony jej zbiór danych X (i etykiety y) na zestaw treningowy i testowy, zgodnie z podanym jako argument procentem (wartość `percentage_train` odpowiadać ma procentowi, jaką częścią oryginalnego zbioru danych ma być zbiór treningowy).\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `split_data`:\n",
    "* Zapoznaj się z dokumentacją funkcji `np.random.choice` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). Zwróć uwagę na parametr `replace`.\n",
    "* Zapoznaj się z trickiem na usunięcie elementów jednej listy z drugiej z wykorzystaniem różnicy zbiorów (więcej o tym możesz przeczytać [TUTAJ](https://stackoverflow.com/questions/3428536/how-do-i-subtract-one-list-from-another/)). Do utworzenia listy indeksów wszystkich rekordów danych możesz użyć funkcji `np.arange` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)).\n",
    "* Zapoznaj się ze sposobami na wyodrębnienie części tablicy w NumPy ([TUTAJ](https://numpy.org/doc/stable/user/basics.indexing.html)). Możesz je filtrować według elementów innej listy!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, percentage_train = 70.0, seed=100):\n",
    "    \"\"\"Funkcja dzieląca losowo dane na zestaw treningowy oraz testowy w zadanej proporcji. \\n\n",
    "    \n",
    "    Argumenty: \\n\n",
    "      - X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "      - y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "      - percentage_train (argument opcjonalny) - jakim procentem wejściowych danych ma być \n",
    "        zestaw treningowy (skalar, float, domyślna wartość: 70). \\n\n",
    "    Zwraca: \\n\n",
    "      - Xtrain - dane treningowe (numpy array, shape = (num_samples * percentage_train, num_features) ), \\n\n",
    "      - ytrain - etykiety do danych treningowych (numpy array, shape = (num_samples * percentage_train,) ), \\n\n",
    "      - Xtest - dane testowe (numpy array, shape = (num_samples * (100-percentage_train), num_features) ), \\n\n",
    "      - ytest - etykiety do danych testowych (numpy array, shape = (num_samples * (100-percentage_train),) ).\"\"\"\n",
    "    \n",
    "    np.random.seed(seed) # Dla zapanowania nad \"losowością\"\n",
    "    num_all_datapoints =  X.shape[0] # Ilość wszystkich danych\n",
    "    num_train_datapoints = int(np.round(X.shape[0]*percentage_train/100)) # Docelowa wielkość zestawu treningowego\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Znając rozmiar danych wejściowych, wygeneruj indices_train - listę indeksów tych rekordów, \n",
    "    # które mają należeć do zestawu treningowego\n",
    "    indices_train = np.random.choice(X.shape[0], size=num_train_datapoints, replace=False)\n",
    "    ind = np.zeros(X.shape[0], dtype=bool) # https://stackoverflow.com/questions/50491630/randomly-split-a-numpy-array\n",
    "    ind[indices_train] = True\n",
    "    indices_test = ~ind\n",
    "    # Wygeneruj zmienne z właściwie podzielonym zbiorem danych: Xtrain, ytrain, Xtest, ytest\n",
    "    Xtrain = X[indices_train]\n",
    "    ytrain = y[indices_train]\n",
    "    Xtest = X[indices_test]\n",
    "    ytest = y[indices_test]\n",
    "    # ------------------------------\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pora sprawdzić, jak działają napisane przez Ciebie funkcje! Uruchom poniższy kod, aby podzielić nasz zbiór danych na zestaw treningowy (powinien domyślnie zawierać 70 elementów) i testowy (pozostałe 30 elementów). Elementy w poszczególnych cechach powinny po normalizacji oscylować wokół 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-0.97023354  1.12689892 -1.11317579 -1.32513741]\n",
      " [ 0.73542816 -1.9493154   0.67340264  0.2589927 ]\n",
      " [-0.81517338  0.68743973 -0.97574668 -0.79709404]\n",
      " [-1.59047416 -1.7295858  -1.18189035 -0.9731085 ]\n",
      " [-0.81517338  0.24798054 -1.2506049  -1.14912295]]\n"
     ]
    }
   ],
   "source": [
    "# Podział danych na część treningową i testową\n",
    "Xtrain, ytrain, Xtest, ytest = split_data(X ,y)\n",
    "print(\"Wymiary danych treningowych: \"+ str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \"+ str(Xtest.shape))\n",
    "\n",
    "# Normalizacja obu zestawów danych\n",
    "Xtrain_norm, _, _ = standarize_data(Xtrain) # pomijamy zwracanie mu i sigma dla danych treningowych\n",
    "Xtest_norm, mu_test, sigma_test = standarize_data(Xtest)\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trening modelu regresji logistycznej\n",
    "\n",
    "Teraz, kiedy mamy już gotowe dane, na których możemy pracować, przejdźmy do najważniejszej rzeczy, czyli napisania funkcji, które utworzą nasz model oparty na regresji logistycznej i pozwolą mu się uczyć!\n",
    "\n",
    "Przypomnijmy, że **hipotezą** $h_\\theta(x)$ (tj. funkcją, która wiąże parametry modelu i dane wejściowe, dając w wyniku predykcje) regresji logistycznej jest: \n",
    "\\begin{equation*}\n",
    "    h_\\theta(x) = g(\\theta^Tx)\n",
    "\\end{equation*}\n",
    "gdzie funkcja $g(z)$ jest to tzw. funkcja **sigmoid** o następującej postaci:\n",
    "\\begin{equation*}\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation*}\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:1280/1*OUOB_YF41M-O4GgZH_F2rw.png' />\n",
    "\n",
    "<font size=\"1\">Grafika: towardsdatascience.com</font>\n",
    "</div>\n",
    "\n",
    "Funkcja sigmoid ma kilka ciekawych właściwości, dzięki którym jest często spotykana przy rozwiązywaniu problemów klasyfikacji: jej wartości zawierają się w przedziale od 0 do 1 (dlatego można je traktować jak prawdopodobieństwo należenia danego rekordu danych do pewnej klasy i np. traktować te dane, dla których sigmoid zwrócił wartość większą niż 0,5, jako należące do tejże klasy), a także jest odwracalna i różniczkowalna, dzięki czemu da się obliczać jej gradient niezbędny w procesie uczenia modelu. Zaimplementuj zatem funkcję `sigmoid`, która zwraca wartość sigmoidu dla dowolnej *numpy array*!\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `sigmoid`: Warto skorzystać z funkcji `np.exp` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.exp.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Funkcja obliczająca wartość sigmoidu dla zadanego argumentu z. \\n\n",
    "    \"\"\"\n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    sigmoid = 1/(1 + np.exp(-z)) \n",
    "    # ------------------------------\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `sigmoid`. Obliczymy, co zwraca ta funkcja, gdy argumentem są same zera: skalar, wektor 1-D oraz macierz 2-D. W każdym przypadku, sigmoid powinien zwrócić stukturę o takich samych wymiarach, jak dane wejściowe, a każdy z jej elementów powinien wynosić 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid dla skalara: 0.5\n",
      "Sigmoid dla wektora: [0.5 0.5 0.5]\n",
      "Sigmoid dla macierzy: [[0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji sigmoidu\n",
    "print(\"Sigmoid dla skalara: \" + str(sigmoid(0)))\n",
    "print(\"Sigmoid dla wektora: \" + str(sigmoid(np.zeros((3)))))\n",
    "print(\"Sigmoid dla macierzy: \" + str(sigmoid(np.zeros((3,3)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako funkcję kosztu $J(\\theta)$ użyjemy **binarnej entropii krzyżowej** (ang. *Binary Cross Entropy*, BCE), często spotykaną przy okazji problemów klasyfikacji binarnej. Jej wartość jest tym większa, im więcej pomyłek popełni klasyfikator: przy zgodności etykiety $y^{(i)}$ i predykcji $h_\\theta(x^{(i)})$, oba człony wyrażenia zerują się. Funkcja ta opisana jest wzorem:\n",
    "\\begin{equation*}\n",
    "\tJ(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log(h_\\theta(x^{(i)})) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)}))]\t\n",
    "\\end{equation*}\n",
    "\n",
    "Jak już wiesz, trening modelu opiera się na znalezieniu optymalnych parametrów $\\theta$, tj. takich, przy których funkcja kosztu jest minimalna. My taką optymalizację przeprowadzimy z wykorzystaniem metody gradientu prostego, która do poprawnego działania musi znać gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ naszej funcji kosztu. Pamiętaj, że **gradient ma taki sam wymiar, jak wektor parametrów $\\theta$**, a zatem składa się z $n$ elementów. W przypadku gradientu fukcji BCE, każdy z jego $n$ elementów można obliczyć z następującego wzoru (pomijam tutaj jego wyprowadzenie):\n",
    "\\begin{equation*}\n",
    "\t\\frac{\\partial J(\\theta)}{\\partial \\theta_n} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)}) \\cdot x^{(i)}_n\n",
    "\\end{equation*}\n",
    "\n",
    "Napisz zatem funkcję `compute_cost_and_gradient`, w której na podstawie danych treningowych i zadanych parametrów $\\theta$, obliczysz koszt BCE i jego gradient.\n",
    "\n",
    "<font size=\"2\">Wskazówki przydatne przy implementacji funkcji `compute_cost_and_gradient`:\n",
    "* Przy obliczaniu kosztu, będzie Ci na pewno potrzebna funkcja `np.log2` z biblioteki NumPy ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.log2.html)) oraz napisana przez Ciebie wcześniej funkcja `sigmoid`.\n",
    "* Zamiast używania pętli `for` do iteracji po wszystkich $m$ elementach, możesz wykonać operacje na macierzach (z wykorzystaniem funkcji `np.dot` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.dot.html))). Pamiętaj jednak o właściwościach mnożenia macierzy - nie każde macierze da się przemnożyć, muszą one mieć zgodne \"wewnętrze\" wymiary (ilość kolumn pierwszej macierzy musi być taka sama, jak ilość wierszy drugiej macierzy; wówczas w wyniku mnożenia otrzymujemy macierz o zgodnych \"zewnętrznych\" wymiarach, tj. o liczbie wierszy jak pierwsza macierz i liczbie kolumn jak druga macierz: [M,N]x[N,1]=[M,1]): w razie potrzeby zmień kolejność mnożonych macierzy albo dokonaj ich transpozycji z użyciem funkcji `np.transpose` ([TUTAJ](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html)).\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_and_gradient(X, y, theta):\n",
    "    \"\"\"Funkcja obliczająca koszt BCE dla regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    theta - zestaw parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: \\n\n",
    "    J - obliczony koszt (skalar, float), \\n\n",
    "    grad - obliczony gradient funkcji kosztu (numpy array, shape=(num_features) ). \"\"\"\n",
    "    \n",
    "    # ------- UZUPEŁNIJ KOD --------\n",
    "    # Oblicz wartość funkcji kosztu\n",
    "     \n",
    "\n",
    "   \n",
    "\n",
    "    # Oblicz wartość funkcji kosztu\n",
    "    m = X.shape[0]\n",
    "    predictions = sigmoid(np.dot(X, theta))\n",
    "    J = (-1/m) * (np.dot(y, np.log(predictions)) + np.dot((1 - y), np.log(1 - predictions)))\n",
    "\n",
    "    # Oblicz gradient\n",
    "    grad = (1/m) * np.dot(X.T, (predictions - y))\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdźmy poprawność implementacji powyższej funkcji: uruchom następującą komórkę, aby wyliczyć przykładowe wartości kosztu i jego gradientu dla zerowych parametrów (kiedy wszystkie elementy $\\theta$ są równe 0), liczone dla całego naszego zbioru danych. Koszt powinien wynosić 1, a gradient [-0.2325,  0.1645, -0.6995, -0.27]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: (0.6931471805599452, array([-0.2325,  0.1645, -0.6995, -0.27  ]))\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji funkcji kosztu\n",
    "print(\"Przykładowy koszt i jego gradient, liczony dla zerowych parmetrów: \"+str(compute_cost_and_gradient(\n",
    "    X,y,np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mając wyznaczoną funkcję kosztu, możemy przejść do jej optymalizacji, czyli doboru takich wartości parametrów $\\theta$, które dają najniższą wartość kosztu (tj. najlepiej odwzorowują dane treningowe). Jak już wspomniano, zrobimy to z wykorzystaniem metody gradientu prostego, według której aktualizacja parametrów odbywa się według poniższego wzoru:\n",
    "\\begin{equation*}\n",
    "    \\theta_n := \\theta_n - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta_n}\n",
    "\\end{equation*}\n",
    "gdzie hiperparametr $\\alpha$ (którego wartość musi być zdefiniowana zanim przejdzie się do treningu modelu), oznacza, jak bardzo gradient funkcji kosztu ma wpływ na nową, zaktualizowaną postać parametrów $\\theta$\n",
    "\n",
    "Napisz zatem funkcję `train_logistic_regression`, w której na podstawie danych treningowych, iteracyjnie liczona jest wartość funkcji kosztu i jego gradient, parametry są aktualizowane zgodnie z metodą gradientu prostego, a ponadto przy każdej itaracji wizualizowana jest nowa wartość kosztu, aby móc ocenić, czy w ramach treningu koszt rzeczywiście spada (ważne - jeśli zaobserwowalibyśmy wzrost kosztu wraz z kolejnymi  iteracjami, oznacza to, że model CORAZ GORZEJ radzi sobie z analizą danych treningowych, a zatem wcale się nie uczy!). \"Szkielet\" tej funkcji został już napisany, wykonanych zostanie 400 iteracji, a stała uczenia może zostać w postaci domyślnej (ustawionej na 0,01).\n",
    "\n",
    "<font size=\"2\">Poprawność implementacji tej funkcji sprawdzimy nieco później.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(X,y,alpha=0.01):\n",
    "    \"\"\"Funkcja realizująca optymalizację funkcji kosztu dla regresji logistycznej\n",
    "    w celu wytrenowania modelu (otrzymania zestawu najlepszych parametrów, theta)\n",
    "    z wykorzystaniem metody gradientu prostego. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane treningowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    y - etykiety (numpy array, shape = (num_samples,) ), \\n\n",
    "    alpha (opcjonalnie) - stała uczenia (skalar, float, domyślnie 0.001). \\n\n",
    "    Zwraca: theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \"\"\"\n",
    "    \n",
    "    # Inicjalizacja\n",
    "    num_iterations = 400 # tyle razy wykonać ma się gradient descent\n",
    "    theta = np.zeros((X.shape[1])) # wstępna inicjalizacja parametrów samymi zerami\n",
    "    Js = np.zeros(num_iterations) # wektor przechowujący dotychczasowe wartości kosztu (do wizualizacji)\n",
    "    \n",
    "    # Uruchomienie metody gradientów prostych\n",
    "    print(\"\\nTrwa trening modelu... \")\n",
    "    for i in range(num_iterations):\n",
    "         # Calculate the cost and gradient using your compute_cost_and_gradient function\n",
    "        J, grad = compute_cost_and_gradient(X, y, theta)\n",
    "        Js[i] = J  # Store the cost in the Js array\n",
    "        \n",
    "        # Update theta using gradient descent\n",
    "        theta -= alpha * grad\n",
    "    print(\"Zakończono. \")\n",
    "    \n",
    "    # Wizualizacja zmian kosztu\n",
    "    plt.figure()\n",
    "    plt.plot(Js)\n",
    "    plt.title(\"Efekty treningu modelu - zmiany w koszcie\")\n",
    "    plt.xlabel(\"Numer iteracji\")\n",
    "    plt.ylabel(\"Wartość funkcji kosztu\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja i poskładanie wszystkiego w całość!\n",
    "\n",
    "Wytrenowany model musi umieć dokonywać predykcji - w tym przypadku, podejmować decyzję, czy analizowany rekord danych zaklasyfikować do klasy pierwszej (etykieta 0) czy drugiej (etykieta 1). Napisz zatem ostatnią w tej części ćwiczenia funkcję, `predict_logistic_regression`, która oblicza funkcję hipotezy dla regresji logistycznej i zwraca odpowiednie etykiety.\n",
    "\n",
    "<font size=\"2\">Wskazówka przydatna przy implementacji funkcji `predict_logistic_regression`: Pamiętaj, że model ma zwrócić etykietę 0 w sytuacji, kiedy prawdopodobieństwo obliczone z wykorzystaniem hipotezy jest mniejsze niż 0,5. Poszukaj funkcji z biblioteki NumPy, która realizuje przybliżanie do najbliższej liczby naturalnej!\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic_regression(X,theta):\n",
    "    \"\"\"Funkcja obliczająca ostateczną predykcję regresji logistycznej. \\n\n",
    "    Argumenty: \\n\n",
    "    X - dane wejściowe (numpy array, shape = (num_samples, num_features) ), \\n\n",
    "    theta - zestaw optymalnych parametrów (numpy array, shape = (num_features,) ). \\n\n",
    "    Zwraca: pred - dokonane predykcje (numpy array, shape = (num_samples,) ). \"\"\"\n",
    "    z = np.dot(X, theta)\n",
    "    pred = sigmoid(z)\n",
    "    return np.round(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź poprawność implementacji funkcji `predict_logistic_regression`. W poniższej komórce wykonujemu tę funkcję dla pierwszych pięciu rekordów oryginalnego zbioru danych i zerowych parametrów. Powinieneś otrzymać w wyniku same zera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Przykładowe predykcje przy zerowych parametrach: [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Testowanie poprawności implementacji predykcji\n",
    "print(\"Przykładowe predykcje przy zerowych parametrach: \"+str(predict_logistic_regression(X[0:5,:],np.zeros((X.shape[1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas uruchomić całość - od właściwego uczenia naszego modelu, aż po dokonanie przezeń predykcji na danych testowych! Uruchom poniższy kod, w którym wykorzystujemy niemal wszystko, co do tej pory udało nam się napisać. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trwa trening modelu... \n",
      "Zakończono. \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABt2klEQVR4nO3dd1xV9f8H8Ne97L2XyBK3KCgooiGoJI7K0Te1LEepZVqW2bDvL1cWat/KTEuzHKnlypWpqThy4MK9cIHiYMve935+fyA3rwy5eOFc4PV8PO5DOPfcc96Hc+W+OJ9xZEIIASIiIqJ6Qi51AURERETaxHBDRERE9QrDDREREdUrDDdERERUrzDcEBERUb3CcENERET1CsMNERER1SsMN0RERFSvMNwQERFRvcJwQ9WWnZ2N0aNHw9nZGTKZDO+9955Gr5fJZJgwYULNFFdHhIaGIjQ0VOoydFpcXBxkMhmWL1+u8Wv3798PmUyG/fv3a70uXdGQ3kMjR46Eubm51GVobOTIkfD09JS6jAaF4YbULF++HDKZrMLH0aNHVet++eWXWL58OcaNG4eVK1fitdde03o99+7dw/Tp03HmzBmtbnf79u2YPn26VrdJRES6QV/qAkg3zZw5E15eXmWWN23aVPX13r170blzZ0ybNq3G6rh37x5mzJgBT09P+Pn5aW2727dvx8KFCyUPOLt27ZJ0/1T38T2k+5YsWQKlUil1GQ0Kww2Vq0+fPggICKh0naSkJLRu3bqWKpJOcXExlEolDA0Ntb7tmtgmNSx8D+k+AwMDqUtocNgsRRor7ccQGxuLv/76S9VkFRcXBwAoKCjAtGnT0LRpUxgZGcHNzQ0fffQRCgoKnrjtWbNmQS6X4/vvv8f+/fvRsWNHAMCoUaNU+1m+fDmmTZsGAwMDJCcnl9nG2LFjYW1tjfz8/HL3MXLkSCxcuBAA1JrcgH/7d/zvf//DvHnz4O3tDSMjI1y6dAkAcOXKFfznP/+Bra0tjI2NERAQgK1bt6ptv7Rp7/Dhw5g0aRIcHBxgZmaGgQMHlqn38f4SpT/bdevW4YsvvkDjxo1hbGyMnj174vr162WOZeHChWjSpAlMTEzQqVMnHDx4sMw2S+spPT+P7+tJ/VGmT58OmUyGq1ev4tVXX4WVlRUcHBzw2WefQQiB+Ph49O/fH5aWlnB2dsbXX39dZhtJSUl444034OTkBGNjY/j6+mLFihVl1ktPT8fIkSNhZWUFa2trjBgxAunp6eXWVZVzUR5PT0+MHDmyzHJt910p/bmV9yjd/6Pvt9JzaWpqil69eiE+Ph5CCHz++edo3LgxTExM0L9/f6SlpVVad2FhIaZOnQp/f39YWVnBzMwMwcHB2Ldvn9rrHt33Tz/9pHqvd+zYESdOnFCtt2zZMshkMpw+fbrMMX755ZfQ09PD3bt3y/0ZnDt3DjKZTO28REdHQyaToUOHDmrr9unTB4GBgVX62T7qzJkzcHBwQGhoKLKzswEAp0+fRp8+fWBpaQlzc3P07NlTrUkdAIqKijBjxgw0a9YMxsbGsLOzwzPPPIPdu3cD+Pf/R3mPx/vP7NixAyEhIbCwsIClpSU6duyI3377TfV8eX1ulEol5s2bhzZt2sDY2BhOTk5488038eDBA41/BlQWr9xQuTIyMpCSkqK2TCaTwc7ODq1atcLKlSvx/vvvo3Hjxvjggw8AAA4ODlAqlXjhhRdw6NAhjB07Fq1atcL58+fx7bff4urVq9i8eXOF+/y///s/fPnll1i8eDHGjBmDxMREzJw5E1OnTsXYsWMRHBwMAOjSpQueeeYZzJw5E2vXrlXrlFxYWIgNGzbgxRdfhLGxcbn7efPNN3Hv3j3s3r0bK1euLHedZcuWIT8/H2PHjoWRkRFsbW1x8eJFdO3aFa6urvjkk09gZmaGdevWYcCAAfjjjz8wcOBAtW288847sLGxwbRp0xAXF4d58+ZhwoQJWLt27RN//rNnz4ZcLsfkyZORkZGBuXPnYtiwYTh27JhqnR9//BETJkxAcHAw3n//fcTFxWHAgAGwsbFB48aNn7gPTQ0ZMgStWrXC7Nmz8ddff2HWrFmwtbXF4sWL0aNHD8yZMwerV6/G5MmT0bFjR3Tr1g0AkJeXh9DQUFy/fh0TJkyAl5cX1q9fj5EjRyI9PR0TJ04EAAgh0L9/fxw6dAhvvfUWWrVqhU2bNmHEiBFlatH0XEhh0KBBas24QMkH+7x58+Do6Ki2fPXq1SgsLMQ777yDtLQ0zJ07F4MHD0aPHj2wf/9+fPzxx7h+/Tq+//57TJ48GUuXLq1wv5mZmfj555/x8ssvY8yYMcjKysIvv/yC8PBwHD9+vEzz7m+//YasrCy8+eabkMlkmDt3LgYNGoSbN2/CwMAA//nPfzB+/HisXr0a7du3L1N3aGgoXF1dy63Fx8cH1tbW+Oeff/DCCy8AAA4ePAi5XI6zZ88iMzMTlpaWUCqVOHLkCMaOHVvVHy8A4MSJEwgPD0dAQAC2bNkCExMTXLx4EcHBwbC0tMRHH30EAwMDLF68GKGhoThw4IAqQE2fPh0REREYPXo0OnXqhMzMTJw8eRKnTp3Cs88+q/o996j09HRMmjRJ7fwtX74cr7/+Otq0aYMpU6bA2toap0+fxs6dO/HKK69UWPubb76J5cuXY9SoUXj33XcRGxuLBQsW4PTp0zh8+DCv9jwtQfSIZcuWCQDlPoyMjNTW9fDwEP369VNbtnLlSiGXy8XBgwfVli9atEgAEIcPH1YtAyDGjx8vhBDigw8+EHK5XCxfvlztdSdOnBAAxLJly8rUGhQUJAIDA9WWbdy4UQAQ+/btq/Q4x48fL8p7+8fGxgoAwtLSUiQlJak917NnT9G2bVuRn5+vWqZUKkWXLl1Es2bNVMtKf4ZhYWFCqVSqlr///vtCT09PpKenq5aFhISIkJAQ1ff79u0TAESrVq1EQUGBavl3330nAIjz588LIYQoKCgQdnZ2omPHjqKoqEi13vLlywUAtW2W1hMbG6t2PKX7etLPatq0aQKAGDt2rGpZcXGxaNy4sZDJZGL27Nmq5Q8ePBAmJiZixIgRqmXz5s0TAMSqVatUywoLC0VQUJAwNzcXmZmZQgghNm/eLACIuXPnqu0nODi4zHugqueivGP08PBQq6/U4+dC25KTk4W7u7to27atyM7OFkL8+35zcHBQe19MmTJFABC+vr5q5/fll18WhoaGasf9eN3FxcVq7x0hSs6Lk5OTeP3111XLSvdtZ2cn0tLSVMu3bNkiAIg///xTbb+NGjUSCoVCtezUqVMV/t98VL9+/USnTp1U3w8aNEgMGjRI6OnpiR07dqhta8uWLZVua8SIEcLMzEwIIcShQ4eEpaWl6Nevn9rPY8CAAcLQ0FDcuHFDtezevXvCwsJCdOvWTbXM19e3zO+vyiiVSvHcc88Jc3NzcfHiRSGEEOnp6cLCwkIEBgaKvLy8Mus/WreHh4fq+4MHDwoAYvXq1Wqv2blzZ7nLSXNslqJyLVy4ELt371Z77Nix44mvW79+PVq1aoWWLVsiJSVF9ejRowcAlLk0LoTAhAkT8N1332HVqlXl/pVekeHDh+PYsWO4ceOGatnq1avh5uaGkJCQKm+nPC+++CIcHBxU36elpWHv3r0YPHgwsrKyVMeVmpqK8PBwXLt2rcyl+bFjx6qauwAgODgYCoUCt27deuL+R40apdaXovSq1c2bNwEAJ0+eRGpqKsaMGQN9/X8vwA4bNgw2NjbVO+gnGD16tOprPT09BAQEQAiBN954Q7Xc2toaLVq0UNUJlHTednZ2xssvv6xaZmBggHfffRfZ2dk4cOCAaj19fX2MGzdObT/vvPOOWh3VORdSUygUePnll5GVlYVNmzbBzMxM7fmXXnoJVlZWqu9Lry68+uqrauc3MDAQhYWFlR6fnp6e6r2jVCqRlpaG4uJiBAQE4NSpU2XWHzJkiNp75vH3GlDyf+3evXtq/39Xr14NExMTvPjii5Uee3BwME6dOoWcnBwAwKFDh9C3b1/4+fnh4MGDAEqu5shkMjzzzDOVbqvUvn37EB4ejp49e2Ljxo0wMjICUPJz3rVrFwYMGIAmTZqo1ndxccErr7yCQ4cOITMzE0DJe/XixYu4du1alfb5+eefY9u2bVi+fLmqr+Hu3buRlZWFTz75pMyV4kf/7z9u/fr1sLKywrPPPqv2e9Lf3x/m5uZlfk+S5tgsReXq1KnTEzsUl+fatWu4fPmyWjB4VFJSktr3v/76K7Kzs/Hjjz+qffhVxZAhQ/Dee+9h9erVmDp1KjIyMrBt2za8//77lf5iqYrHR4pdv34dQgh89tln+Oyzz8p9TVJSktrleXd3d7XnSz9AqtKm/qTXlgakx5s99PX1a2w+jcdrsrKygrGxMezt7cssT01NVX1/69YtNGvWDHK5+t9SrVq1Uj1f+q+Li0uZeUxatGih9n11zoW2pKWlobCwUPW9iYmJWiipyP/93/9h7969+Ouvv+Dt7V3m+fJ+tgDg5uZW7vInvYdWrFiBr7/+GleuXEFRUZFqeXkjIKvyPn322Wfh4uKC1atXo2fPnlAqlfj999/Rv39/WFhYVFpLcHAwiouLERUVBTc3NyQlJSE4OBgXL15UCzetW7eGra1tpdsCgPz8fPTr1w/+/v5Yt26dWvhLTk5Gbm5umfcMUPJ+UyqViI+PR5s2bTBz5kz0798fzZs3h4+PD3r37o3XXnsN7dq1K/PanTt3YsaMGZgyZYpamCv9w8rHx+eJdT/q2rVryMjIKNM8Werx35OkOYYb0iqlUom2bdvim2++Kff5x39Zd+3aFWfOnMGCBQswePDgKv1yK2VjY4PnnntOFW42bNiAgoICvPrqq091DEDJh9ajSodxTp48GeHh4eW+5vGgoaenV+56Qogn7v9pXvu4ioKeQqHQaDvl1aTNOquqOufiUZX9PCo6nlKDBg1SXWkCgBEjRjxxcsHNmzdjzpw5+Pzzz9G7d+9y16lov9X5+a5atQojR47EgAED8OGHH8LR0RF6enqIiIhQu8qpyT709PTwyiuvYMmSJfjhhx9w+PBh3Lt3r0r/1wICAmBsbIx//vkH7u7ucHR0RPPmzREcHIwffvgBBQUFOHjwYJX7SRkZGaFv377YsmULdu7cieeee65Kr3tct27dcOPGDWzZsgW7du3Czz//jG+//RaLFi1Su0oZGxuLYcOG4dlnn8WsWbOqta/HKZVKODo6YvXq1eU+X9Efh1R1DDekVd7e3jh79ix69uxZpasnTZs2xdy5cxEaGorevXsjMjJS7S/BJ21j+PDh6N+/P06cOKHq8NimTZsn7lfTKzull7gNDAwQFham0WtrgoeHB4CSqxjdu3dXLS8uLkZcXJzaX5+lf4k/PuqoKs1j2uDh4YFz585BqVSqXb25cuWK6vnSfyMjI5Gdna129SYmJkZte097LmxsbModgXXr1i21pozyfP3112pXNBo1alTp+levXsWIESMwYMAAfPrppxrXWh0bNmxAkyZNsHHjRrX3+dPORzV8+HB8/fXX+PPPP7Fjxw44ODhUGC4fZWhoqBrJ5+7urmr2Cg4ORkFBAVavXo3ExERVB/QnkclkWL16Nfr374+XXnoJO3bsUI0Wc3BwgKmpaZn3DFDyfpPL5Wp/YNna2mLUqFEYNWoUsrOz0a1bN0yfPl0VbvLy8jBo0CBYW1vj999/L3P1sfQq3IULFyoN1I/z9vbGnj170LVr1zJ/SJF2sM8NadXgwYNx9+5dLFmypMxzeXl5qnb3R7Vr1w7bt2/H5cuX8fzzzyMvL0/1XGnfhIqGA/fp0wf29vaYM2cODhw4UOWrNk/a7uMcHR0RGhqKxYsX4/79+2WeL29Iek0KCAiAnZ0dlixZguLiYtXy1atXl2myKP0F/M8//6iWKRQK/PTTT7VSa9++fZGQkKA2Sqy4uBjff/89zM3NVf2j+vbti+LiYvz4449qdX7//fdq23vac+Ht7Y2jR4+qNS9t27YN8fHxTzwWf39/hIWFqR6VzfOUnZ2NgQMHwtXVFStWrHjqptKqKr0S8+iVl2PHjiEqKuqpttuuXTu0a9cOP//8M/744w8MHTpUrUmoMsHBwTh27Bj27dunCjf29vZo1aoV5syZo1qnqgwNDbFx40Z07NgRzz//PI4fPw6g5Nh79eqFLVu2qE19kJiYiN9++w3PPPMMLC0tAUCt6RQAzM3N0bRpU7UpK9566y1cvXoVmzZtKrcvW69evWBhYYGIiIgyU09UdnVt8ODBUCgU+Pzzz8s8V1xcXOXfS1QxXrmhcu3YsUP1l/WjunTpUulft6+99hrWrVuHt956C/v27UPXrl2hUChw5coVrFu3Dn///Xe5fXk6d+6MLVu2oG/fvvjPf/6DzZs3w8DAAN7e3rC2tsaiRYtgYWEBMzMzBAYGqvoOGBgYYOjQoViwYAH09PSq3G/H398fAPDuu+8iPDwcenp6GDp0aKWvWbhwIZ555hm0bdsWY8aMQZMmTZCYmIioqCjcuXMHZ8+erdK+tcHQ0BDTp0/HO++8gx49emDw4MGIi4vD8uXL4e3trfZB2qZNG3Tu3BlTpkxBWloabG1tsWbNGrVQVJPGjh2LxYsXY+TIkYiOjoanpyc2bNiAw4cPY968eaordc8//zy6du2KTz75BHFxcWjdujU2btyIjIyMMtt8mnMxevRobNiwAb1798bgwYNx48YNrFq1qty+ME9jxowZuHTpEv7v//4PW7ZsUXvO29sbQUFBWt1fqeeeew4bN27EwIED0a9fP8TGxmLRokVo3bq1ah6Y6ho+fDgmT54MABo1/wYHB+OLL75AfHy8Wojp1q0bFi9eDE9PT42nLzAxMcG2bdvQo0cP9OnTBwcOHICPjw9mzZqF3bt345lnnsHbb78NfX19LF68GAUFBZg7d67q9a1bt0ZoaCj8/f1ha2uLkydPYsOGDaqpJf766y/8+uuvePHFF3Hu3DmcO3dO9Vpzc3MMGDAAlpaW+PbbbzF69Gh07NgRr7zyCmxsbHD27Fnk5uaWO5cTAISEhODNN99EREQEzpw5g169esHAwADXrl3D+vXr8d133+E///mPRj8PeoxEo7RIR1U2FByPDfssbyi4ECXDfOfMmSPatGkjjIyMhI2NjfD39xczZswQGRkZqvXwyFDwUlu2bBH6+vpiyJAhqmGnW7ZsEa1btxb6+vrlDj09fvy4ACB69epV5eMsLi4W77zzjnBwcBAymUw1LLx0eOxXX31V7utu3Lghhg8fLpydnYWBgYFwdXUVzz33nNiwYUOZn+GJEyfUXlvesOSKhoKvX79e7bWldT1+7PPnzxceHh7CyMhIdOrUSRw+fFj4+/uL3r17l6k7LCxMGBkZCScnJ/Hpp5+K3bt3azQUPDk5WW35o8NyHxUSEiLatGmjtiwxMVGMGjVK2NvbC0NDQ9G2bdtyhxCnpqaK1157TVhaWgorKyvx2muvidOnT5d77FU5FxUNd//666+Fq6urMDIyEl27dhUnT57U+lDwESNGVPj/qHQoekXvt4reB+W9tx6vW6lUii+//FL1vmjfvr3Ytm1bmeHIlb3XAYhp06aVWX7//n2hp6cnmjdvrtHPIjMzU+jp6QkLCwtRXFysWr5q1SoBQLz22mtV2k5577mUlBTRunVr4ezsLK5duyaEKBlaHh4eLszNzYWpqano3r27OHLkiNrrZs2aJTp16iSsra2FiYmJaNmypfjiiy9EYWGhEKLy34WP/hyFEGLr1q2iS5cuwsTERFhaWopOnTqJ33//Xa3ux18jhBA//fST8Pf3FyYmJsLCwkK0bdtWfPTRR+LevXtV+nlQxWRC1GDPP6JacPbsWfj5+eHXX3+tkZt31iVKpRIODg4YNGhQuU2DRE8jJSUFLi4umDp1aoUj1Yh0AfvcUJ23ZMkSmJubY9CgQVKXUqvy8/PLtOv/+uuvSEtL0+ptBIhKLV++HAqFosH/EUG6j31uqM76888/cenSJfz000+YMGFCmYnR6rujR4/i/fffx0svvQQ7OzucOnUKv/zyC3x8fPDSSy9JXR7VI3v37sWlS5fwxRdfYMCAATU2lxKRtrBZiuosT09PJCYmIjw8HCtXrnziZGL1TVxcHN59910cP35c1VG4b9++mD17doWTgxFVR2hoKI4cOYKuXbti1apVNTJBIpE2MdwQERFRvcI+N0RERFSvMNwQERFRvdIgOxQrlUrcu3cPFhYWtTZrKBERET0dIQSysrLQqFGjMrfDeFSDDDf37t0rcwNHIiIiqhvi4+MrndW6QYab0lE18fHxqvuMEBERkW7LzMyEm5vbE0fHNshwU9oUZWlpyXBDRERUxzypSwk7FBMREVG9wnBDRERE9QrDDREREdUrDDdERERUrzDcEBERUb3CcENERET1ik6Em4ULF8LT0xPGxsYIDAzE8ePHK1w3NDQUMpmszKNfv361WDERERHpKsnDzdq1azFp0iRMmzYNp06dgq+vL8LDw5GUlFTu+hs3bsT9+/dVjwsXLkBPTw8vvfRSLVdOREREukjycPPNN99gzJgxGDVqFFq3bo1FixbB1NQUS5cuLXd9W1tbODs7qx67d++Gqakpww0REREBkDjcFBYWIjo6GmFhYaplcrkcYWFhiIqKqtI2fvnlFwwdOhRmZmYVrlNQUIDMzEy1BxEREdVPkoablJQUKBQKODk5qS13cnJCQkLCE19//PhxXLhwAaNHj650vYiICFhZWakevGkmERFR/SV5s9TT+OWXX9C2bVt06tSp0vWmTJmCjIwM1SM+Pr6WKiQiIqLaJmm4sbe3h56eHhITE9WWJyYmwtnZudLX5uTkYM2aNXjjjTeeuB8jIyPVTTJr8maZCqXA6dsPkF+kqJHtExER0ZNJGm4MDQ3h7++PyMhI1TKlUonIyEgEBQVV+tr169ejoKAAr776ak2XWWUvLDiEgT8cQdTNVKlLISIiarAkb5aaNGkSlixZghUrVuDy5csYN24ccnJyMGrUKADA8OHDMWXKlDKv++WXXzBgwADY2dnVdskVatfYCgBwICZZ4kqIiIgaLn2pCxgyZAiSk5MxdepUJCQkwM/PDzt37lR1Mr59+zbkcvUMFhMTg0OHDmHXrl1SlFyhkOaO+P14PPbHJAFoI3U5REREDZJMCCGkLqK2ZWZmwsrKChkZGVrtf5OVX4T2M3ejWCmwf3IoPO0rHp5OREREmqnq57fkzVL1iYWxAQI8bQDg4dUbIiIiqm0MN1oW2sIRALD/KvvdEBERSYHhRstCWzgAAKJupHJIOBERkQQYbrSshZMFnC2NUVCsxFEOCSciIqp1DDdaJpPJVFdvDrBpioiIqNYx3NQAVbjhfDdERES1juGmBnRtag99uQw3U3JwOzVX6nKIiIgaFIabGmBhbAB/j4dDwq9ySDgREVFtYripIaoh4WyaIiIiqlUMNzWktN/NkRspHBJORERUixhuakhLZws4WRohv0iJ47FpUpdDRETUYDDc1BCZTIaQ5iVXb9g0RUREVHsYbmrQv/1u2KmYiIiotjDc1KBnmv07JPxmcrbU5RARETUIDDc1yNLYAJ2b2AEAIi/z6g0REVFtYLipYT1blTRN7bmcKHElREREDQPDTQ0La+UEADh56wHScwslroaIiKj+Y7ipYW62pmjhZAGFUnDUFBERUS1guKkFYa3ZNEVERFRbGG5qQc+HTVMHYpJRWKyUuBoiIqL6jeGmFvg1toa9uSGyCopxIo6zFRMREdUkhptaIJfL0KNlSdPU7ktsmiIiIqpJDDe1pLRpKvJKIoQQEldDRERUfzHc1JLgZvYw1JcjPi0PVxM5WzEREVFNYbipJaaG+ujqXTJbMUdNERER1RyGm1oU1vph0xTDDRERUY1huKlFPVuWhJvT8elIyS6QuBoiIqL6ieGmFjlbGaOtqxWEAPbyRppEREQ1guGmlj37sGlq58UEiSshIiKqnxhuallvH2cAwKFrKcjKL5K4GiIiovqH4aaWNXM0RxMHMxQqlNjHG2kSERFpHcNNLZPJZOjdpuTqzd8X2DRFRESkbQw3EihtmtoXk4T8IoXE1RAREdUvDDcSaOtqhUZWxsgtVODgtRSpyyEiIqpXGG4kIJPJEP7w6s2OC/clroaIiKh+YbiRSGm/mz2XElGkUEpcDRERUf3BcCORAE9b2JsbIjO/GEdvpkpdDhERUb3BcCMRPbkMz7YuuXqzk6OmiIiItIbhRkKlo6b+vpgIhVJIXA0REVH9wHAjoaAmdrAw1kdKdgFO3X4gdTlERET1AsONhAz15Qhr9fBeU2yaIiIi0gqGG4mFt/m3340QbJoiIiJ6Wgw3Egtp7gBTQz3cTc/Dmfh0qcshIiKq8xhuJGZiqKdqmvrzLCf0IyIieloMNzrguXYuAIDt5+9DyVFTRERET4XhRgeEtHCAhbE+EjLzcfIWR00RERE9DYYbHWCkr4deDyf023bunsTVEBER1W06EW4WLlwIT09PGBsbIzAwEMePH690/fT0dIwfPx4uLi4wMjJC8+bNsX379lqqtmY851vaNJXACf2IiIieguThZu3atZg0aRKmTZuGU6dOwdfXF+Hh4UhKSip3/cLCQjz77LOIi4vDhg0bEBMTgyVLlsDV1bWWK9euZ5raw9rUACnZBTjGe00RERFVm+Th5ptvvsGYMWMwatQotG7dGosWLYKpqSmWLl1a7vpLly5FWloaNm/ejK5du8LT0xMhISHw9fWt5cq1y0BPrrpT+J9smiIiIqo2ScNNYWEhoqOjERYWploml8sRFhaGqKiocl+zdetWBAUFYfz48XBycoKPjw++/PJLKBSKCvdTUFCAzMxMtYcuet63EQBgx4UEFCmUEldDRERUN0kablJSUqBQKODk5KS23MnJCQkJ5d+O4ObNm9iwYQMUCgW2b9+Ozz77DF9//TVmzZpV4X4iIiJgZWWleri5uWn1OLQl0MsW9uaGSM8twuHrKVKXQ0REVCdJ3iylKaVSCUdHR/z000/w9/fHkCFD8N///heLFi2q8DVTpkxBRkaG6hEfH1+LFVedvp4cfXxKOhZvO8cJ/YiIiKpD0nBjb28PPT09JCYmqi1PTEyEs7Nzua9xcXFB8+bNoaenp1rWqlUrJCQkoLCwsNzXGBkZwdLSUu2hq0on9Pv7YgIKiituaiMiIqLySRpuDA0N4e/vj8jISNUypVKJyMhIBAUFlfuarl274vr161Aq/+2TcvXqVbi4uMDQ0LDGa65pHT1t4WRphKz8YvxzlU1TREREmpK8WWrSpElYsmQJVqxYgcuXL2PcuHHIycnBqFGjAADDhw/HlClTVOuPGzcOaWlpmDhxIq5evYq//voLX375JcaPHy/VIWiVXC7Dc+1KOhZvPnNX4mqIiIjqHn2pCxgyZAiSk5MxdepUJCQkwM/PDzt37lR1Mr59+zbk8n8zmJubG/7++2+8//77aNeuHVxdXTFx4kR8/PHHUh2C1g1s74pfDsViz6VEZOYXwdLYQOqSiIiI6gyZEKLBTYebmZkJKysrZGRk6GT/GyEEnv32H1xPysbcF9thcEfdHN1FRERUm6r6+S15sxSVJZPJMLB9yYzLm06zaYqIiEgTDDc6qr9fSb+bo7GpuJeeJ3E1REREdQfDjY5qbGOKQC9bCAFsOcPbMRAREVUVw40O+7dp6g4aYNcoIiKiamG40WF92rrAUF+Oq4nZuHhPN++HRUREpGsYbnSYlYkBnm1VMiR+MzsWExERVQnDjY4b8LBpasvZeyjmncKJiIieiOFGx4U0d4CNqQGSswpw5Eaq1OUQERHpPIYbHWeoL1fdjoFz3hARET0Zw00dMLBDSdPUzgsJyC4olrgaIiIi3cZwUwe0d7NGE3sz5BUp8Nc5znlDRERUGYabOkAmk+GlgJL7S607eUfiaoiIiHQbw00d8WIHV+jJZYi+9QDXk7KlLoeIiEhnMdzUEY6WxujewgEAsD46XuJqiIiIdBfDTR1S2jT1R/RdFHHOGyIionIx3NQhPVo6wt7cECnZBTgQkyx1OURERDqJ4aYOMdCTq26mue4km6aIiIjKw3BTx5Q2Te29koTkrAKJqyEiItI9DDd1THMnC/i5WaNYKbDpNIeFExERPY7hpg4a/MicN0IIiashIiLSLQw3ddBzvi4wNpDjelI2TsenS10OERGRTmG4qYMsjQ3Qt60LAOD3Y7clroaIiEi3MNzUUa90cgcA/HnuHjLyiiSuhoiISHcw3NRR/h42aOFkgfwiJTadYsdiIiKiUgw3dZRMJsOwziVXb1Yfu82OxURERA8x3NRhA9q7wsRAD9eSsnEi7oHU5RAREekEhps6zNLYAP39GgEAVh+7JXE1REREuoHhpo4bFugBANhxPgFpOYUSV0NERCQ9hps6rm1jK7RrbIVChRIbonm/KSIiIoabemBYYEnH4t+O3YZSyY7FRETUsDHc1APP+zaChZE+4lJzceRGqtTlEBERSUpf0xfMnDmz0uenTp1a7WKoekwN9TGogytWRN3C6mO38Ewze6lLIiIikozG4WbTpk1q3xcVFSE2Nhb6+vrw9vZmuJHIK4EeWBF1C7suJeJ+Rh5crEykLomIiEgSGoeb06dPl1mWmZmJkSNHYuDAgVopijTXwtkCgV62OBabhlVHb+HD8JZSl0RERCQJrfS5sbS0xIwZM/DZZ59pY3NUTaO6egIo6VicX6SQthgiIiKJaK1DcUZGBjIyMrS1OaqGsFZOcLU2wYPcImw9e0/qcoiIiCShcbPU/Pnz1b4XQuD+/ftYuXIl+vTpo7XCSHP6enIMD/JAxI4rWH44Di/5N4ZMJpO6LCIiolqlcbj59ttv1b6Xy+VwcHDAiBEjMGXKFK0VRtUzpKMbvt1zFZfuZ+JE3AN08rKVuiQiIqJapXG4iY2NrYk6SEusTQ0xsH1j/H78NpYfiWW4ISKiBkfjPjevv/46srKyyizPycnB66+/rpWi6OmM7OIJAPj7YiLupudJWwwREVEt0zjcrFixAnl5ZT8w8/Ly8Ouvv2qlKHo6LZwt0MXbDgqlwMoo3i2ciIgaliqHm8zMTGRkZEAIgaysLGRmZqoeDx48wPbt2+Ho6FiTtZIGRnX1AgCsOXEbeYUcFk5ERA1HlfvcWFtbQyaTQSaToXnz5mWel8lkmDFjhlaLo+rr0dIRbrYmiE/Lw+Yzd/FyJ3epSyIiIqoVVQ43+/btgxACPXr0wB9//AFb2387qhoaGsLDwwONGjWqkSJJc3pyGUYEeWLWX5fx88GbGBLgBrmcw8KJiKj+q3K4CQkJAVAyWsrd3Z3zp9QBQzu547vIa7iRnIO9V5IQ1tpJ6pKIiIhqnMYdirt3747XX38dBQUFastTUlLQpEkTrRVGT8/cSB+vBJY0R/108KbE1RAREdUOjcNNXFwcDh8+jODgYCQkJKiWKxQK3LpVvZE5CxcuhKenJ4yNjREYGIjjx49XuO7y5ctVfX9KH8bGxtXab0MwqosXDPRkOB6bhjPx6VKXQ0REVOM0DjcymQw7d+5E48aN4e/vjxMnTjxVAWvXrsWkSZMwbdo0nDp1Cr6+vggPD0dSUlKFr7G0tMT9+/dVj+qGqobA2coYL/i6AgB++ueGxNUQERHVPI3DjRAC5ubm2LhxI4YPH46QkBCsWrWq2gV88803GDNmDEaNGoXWrVtj0aJFMDU1xdKlSyt8jUwmg7Ozs+rh5MS+JJUZ262kuXDnhQTcSs2RuBoiIqKaVa0rN6UiIiLw008/YcyYMdW6r1RhYSGio6MRFhb2b0FyOcLCwhAVFVXh67Kzs+Hh4QE3Nzf0798fFy9e1HjfDUkLZwuEtnCAUgC/HOLtM4iIqH6r1pWbR7366qvYu3cvtm/frvHOU1JSoFAoylx5cXJyUuvP86gWLVpg6dKl2LJlC1atWgWlUokuXbrgzp07Fe6noKBAbdLBzMxMjWut68YGl1y9WXcyHmk5hRJXQ0REVHM0DjdKpbLMTMRBQUE4e/Ys9u7dq7XCKhIUFIThw4fDz88PISEh2LhxIxwcHLB48eIKXxMREQErKyvVw83Nrcbr1DVB3nbwcbVEfpESq46yjxIREdVfGoebvLw85Obmqr6/desW5s2bh7Nnz6rmwqkqe3t76OnpITExUW15YmIinJ2dq7QNAwMDtG/fHtevX69wnSlTpiAjI0P1iI+P16jO+kAmk2HMw6s3K47E8ZYMRERUb2kcbvr376+6QWZ6ejoCAwPx9ddfo3///vjxxx812pahoSH8/f0RGRmpWqZUKhEZGYmgoKAqbUOhUOD8+fNwcXGpcB0jIyNYWlqqPRqifm1d0NjGBKk5hVh74rbU5RAREdUIjcPNqVOnEBwcDADYsGEDnJyccOvWLfz666+YP3++xgVMmjQJS5YswYoVK3D58mWMGzcOOTk5GDVqFABg+PDhap2VZ86ciV27duHmzZs4deoUXn31Vdy6dQujR4/WeN8Njb6eHG+FeAMAFv9zE4XFSokrIiIi0r4q336hVG5uLiwsLAAAu3btwqBBgyCXy9G5c+dqzTczZMgQJCcnY+rUqUhISICfnx927typ6mR8+/ZtyOX/ZrAHDx5gzJgxSEhIgI2NDfz9/XHkyBG0bt1a4303RP/xb4z5kddwPyMfG0/dwVDeUJOIiOoZmXh8+NMTtGvXDqNHj8bAgQPh4+ODnTt3IigoCNHR0ejXr1+Fo5x0SWZmJqysrJCRkdEgm6h+PngTs/66DA87U0ROCoG+nsYX8IiIiGpdVT+/Nf5Umzp1KiZPngxPT0906tRJ1Tdm165daN++ffUrplrzSqA7bM0McSs1F9vO3Ze6HCIiIq3SONz85z//we3bt3Hy5En8/fffquU9e/bEt99+q9XiqGaYGurjjWe8AAAL912HUqnRxTsiIiKdVq32CGdnZ7Rv3x737t1TTZ7XqVMntGzZUqvFUc15LcgDFsb6uJaUjV2XdL8pkYiIqKqqNYnfzJkzYWVlBQ8PD3h4eMDa2hqff/45lEqOvqkrLI0NMCLIEwCwYN/1MjNPExER1VUah5v//ve/WLBgAWbPno3Tp0/j9OnT+PLLL/H999/js88+q4kaqYa8/owXTAz0cOFuJvZfTZa6HCIiIq3QeLRUo0aNsGjRIrzwwgtqy7ds2YK3334bd+/e1WqBNaGhj5Z61Kxtl/DzoVj4uVlj09td1G6MSkREpEtqbLRUWlpauX1rWrZsibS0NE03RxJ7M8QbxgZynIlPx/4YXr0hIqK6T+Nw4+vriwULFpRZvmDBAvj6+mqlKKo9DhZGqr433+y+yr43RERU52k8Q/HcuXPRr18/7NmzRzXHTVRUFOLj47F9+3atF0g1b2y3Jlh59BbO383A7kuJ6NWmajctJSIi0kUaX7kJCQnB1atXMXDgQKSnpyM9PR2DBg1CTEyM6p5TVLfYmRthVFdPAMC3e65x3hsiIqrTNO5QvG/fPnTv3r3c5xYuXIjx48drpbCaxA7FZaXnFiJ4zj5kFRTjh2Ed0LdtxXdZJyIikkKNdSgeNGgQoqOjyyz/7rvv1O7eTXWLtakhXn84a/G3u69Cwas3RERUR2kcbr766iv06dMHV65cUS37+uuvMXXqVPz1119aLY5q1+vPeMHy4azFf53nPaeIiKhu0jjcjB49GpMnT0ZYWBji4uIwZ84czJw5E9u3b2efmzrOysQAY4KbAADm7bmKYgVnnCYiorpH49FSAPDRRx8hNTUVAQEBUCgU+Pvvv9G5c2dt10YSGPWMF5YejsXN5Bz8ceoOhnR0l7okIiIijVQp3MyfP7/MMldXV5iamqJbt244fvw4jh8/DgB49913tVsh1SpzI32M794Us/66jG93X0N/P1cYG+hJXRYREVGVVWm0lJeXV9U2JpPh5s2bT11UTeNoqcoVFCvQ438HcDc9D5/0aYm3QrylLomIiKjKn99VunITGxurtcJI9xnp6+GDXs0xad1Z/LDvOoZ2dIO1qaHUZREREVWJxh2KqWHo7+eKls4WyMwvxo/7b0hdDhERUZUx3FC59OQyfNy75Aapy47E4V56nsQVERERVQ3DDVUotIUDAr1sUVisxLe7r0pdDhERUZUw3FCFZDIZPulTcvXmj1N3EJOQJXFFRERET8ZwQ5Vq726D3m2coRTA7B2XpS6HiIjoiao0WurcuXPw8fGBXC7HuXPnKl23Xbt2WimMdMdHvVtgz+VE7ItJxoGryQhp7iB1SURERBWq0jw3crkcCQkJcHR0hFwuh0wmw6MvK/1eJpNBoVDUaMHawHluNDfzz0tYejgWzRzNsWNiMPT1eNGPiIhql9bnuXFwcFB9TQ3PxJ7NsPH0HVxLysbvJ+LxWmcPqUsiIiIqV5XCjYeHR7lfU8NhZWqA98OaY9rWi/hmVwxe8G0EKxMDqcsiIiIqo0rhZuvWrejTpw8MDAywdevWStc1NzdHy5Yt0ahRI60USLrjlUB3rDx6C9eTsrFg7zX8t19rqUsiIiIqo1p9bp5ET08Pc+fOxfvvv6+VIrWNfW6qb19MEkYtOwEDPRl2vx8CT3szqUsiIqIGoqqf31XqFapUKuHo6Kj6urJHfn4+lixZgrlz52rnSEindG/hiJDmDihSCHy5nUPDiYhI92h9yIuhoSFefPFFvPzyy9reNOmI/+vXCnpyGXZdSsTBa8lSl0NERKRG43ATERGBpUuXllm+dOlSzJkzBwBgYWGBb7755umrI53UzMlCNVpq2taLKCxWSlwRERHRvzQON4sXL0bLli3LLG/Tpg0WLVqklaJI973/bHPYmxviZnIOfjnE6QGIiEh3aBxuEhIS4OLiUma5g4MD7t+/r5WiSPdZmRhgSp9WAIDv917jXcOJiEhnaBxu3NzccPjw4TLLDx8+zOHfDcygDq4I8LBBbqECX/zFzsVERKQbNA43Y8aMwXvvvYdly5bh1q1buHXrFpYuXYr3338fY8aMqYkaSUfJZDLM7O8DuQz46/x9HLqWInVJREREVZvE71EffvghUlNT8fbbb6OwsBAAYGxsjI8//hiffPKJ1gsk3da6kSWGB3li+ZE4TNt6ATsmdoOhPu87RURE0qnSJH7lyc7OxuXLl2FiYoJmzZrByMhI27XVGE7ip10ZeUXo+fV+pGQX4uPeLTEu1FvqkoiIqB7S6iR+j9q3bx+AktssdOzYET4+Pqpgs3DhwmqWS3WZlYkBPnnYufi7yKu4nZorcUVERNSQaRxuBg0ahOjo6DLLv/vuO0yZMkUrRVHd82IHVwQ1sUN+kRL/3Xwe1bwgSERE9NQ0DjdfffUV+vTpgytXrqiWff3115g6dSr++usvrRZHdYdMJsMXA31gqC/HwWsp2Hr2ntQlERFRA6VxuBk9ejQmT56MsLAwxMXFYc6cOZg5cya2b9+O4ODgmqiR6ogmDuZ4t0dTAMDMPy/hQU6hxBUREVFDpPFoKQD46KOPkJqaioCAACgUCvz999/o3LmztmujOmhsN29sPXsPVxOz8eX2y/jqJV+pSyIiogamSuFm/vz5ZZa5urrC1NQU3bp1w/Hjx3H8+HEAwLvvvqvdCqlOMdSXI2JQW7z4YxTWR9/BwA6u6OJtL3VZRETUgFRpKLiXl1fVNiaT4ebNm09dVE3jUPCa93+bz2PV0dvwsjfDjonBMDbQk7okIiKq46r6+V2lKzexsbwxImnmo94tsetiImJTcvDt7quY0reV1CUREVEDwalkqUZYGhvgy4FtAQBLDt7EqdsPJK6IiIgaCo3DjUKhwC+//IJXXnkFYWFh6NGjh9qjOhYuXAhPT08YGxsjMDBQ1X/nSdasWQOZTIYBAwZUa79Us8JaO2Fge1coBfDh+rPIL1JIXRIRETUAGoebiRMnYuLEiVAoFPDx8YGvr6/aQ1Nr167FpEmTMG3aNJw6dQq+vr4IDw9HUlJSpa+Li4vD5MmTOfxcx017vjXszY1wIzkH8/Zck7ocIiJqADS+t5S9vT1+/fVX9O3bVysFBAYGomPHjliwYAEAQKlUws3NDe+8806FN+JUKBTo1q0bXn/9dRw8eBDp6enYvHlzlffJDsW1a9fFBIxdGQ25DPhjXBe0d7eRuiQiIqqDauzeUoaGhmjatOlTFVeqsLAQ0dHRCAsL+7cguRxhYWGIioqq8HUzZ86Eo6Mj3njjDa3UQTWrVxtnDPBrVNI8teEcm6eIiKhGaRxuPvjgA3z33XdauXdQSkoKFAoFnJyc1JY7OTkhISGh3NccOnQIv/zyC5YsWVLl/RQUFCAzM1PtQbVr2vNtYG9uhOtJ2fh2z1WpyyEionpM4xmKDx06hH379mHHjh1o06YNDAwM1J7fuHGj1op7XFZWFl577TUsWbIE9vZVnxguIiICM2bMqLG66MlszAzxxUAfvLkyGj/9cxM9WjgisImd1GUREVE9pHG4sba2xsCBA7Wyc3t7e+jp6SExMVFteWJiIpydncusf+PGDcTFxeH5559XLVMqlQAAfX19xMTEwNvbu8zrpkyZgkmTJqm+z8zMhJubm1aOgaouvI0zXvJvjPXRdzBp3VnseC8YlsYGT34hERGRBjQON8uWLdPazg0NDeHv74/IyEjVcG6lUonIyEhMmDChzPotW7bE+fPn1Zb93//9H7KysvDdd99VGFiMjIxgZGSktbqp+qa90AZHY1MRn5aH6Vsv4pvBflKXRERE9Uy1bpypTZMmTcKIESMQEBCATp06Yd68ecjJycGoUaMAAMOHD4erqysiIiJgbGwMHx8ftddbW1sDQJnlpJvMjfTx7WA/DF4chY2n7iKslRP6tnWRuiwiIqpHNA43Xl5ekMlkFT6v6b2lhgwZguTkZEydOhUJCQnw8/PDzp07VZ2Mb9++DbmcEynXJwGetng7tCkW7LuOTzedh7+HDZwsjaUui4iI6gmN57n57rvv1L4vKirC6dOnsXPnTnz44YcVzk2jSzjPjfSKFEoM+uEIzt/NQHAze6wY1QlyecWhmYiISKs3znzUxIkTy12+cOFCnDx5UtPNUQNloCfHt0P88Nz3B3HwWgp+ORSLMd2aSF0WERHVA1pr7+nTpw/++OMPbW2OGoCmjub47LnWAIA5O6/gbHy6tAUREVG9oLVws2HDBtja2mprc9RAvNLJHX3bOqNYKTDh91PIzC+SuiQiIqrjNG6Wat++vVqHYiEEEhISkJycjB9++EGrxVH9J5PJEDGoHc7dyUB8Wh6mbDyPBS+3r7TTOhERUWU0Djel89GUksvlcHBwQGhoKFq2bKmtuqgBsTIxwPyX22Pwoij8de4+gpvaY2gnd6nLIiKiOqpK4WbSpEn4/PPPYWZmhu7duyMoKKjMbReInkYHdxtMDm+B2TuuYPqfF9HBwwbNnSykLouIiOqgKvW5+f7775GdnQ0A6N69Ox48eFCjRVHDNDa4Cbo1d0B+kRLjVkUju6BY6pKIiKgOqtKVG09PT8yfPx+9evWCEAJRUVGwsbEpd91u3bpptUBqOORyGb4Z7It+8w/iRnIOPv7jHPvfEBGRxqo0id/mzZvx1ltvISkpCTKZDBW9RCaTQaFQaL1IbeMkfrot+lYahiw+imKlwP/1a4XRwZz/hoiIqv75rdEMxdnZ2bC0tERMTAwcHR3LXcfKykrzamsZw43uW344FtP/vAQ9uQy/j+mMTl6cZoCIqKGr6ue3RvPcmJubY9++ffDy8oKVlVW5DyJtGNHFE/39GkGhFBj/2ykkZeZLXRIREdURGk/iFxISAn19yW8mTvVcyfw3bdHCyQLJWQWY8NtpFCmUUpdFRER1AG+3TTrL1FAfP77aARZG+jgel4bZO65IXRIREdUBDDek05o4mOOrl3wBAL8cisWWM3clroiIiHQdww3pvN4+zhgX6g0A+GjDOd5gk4iIKqVxuMnIyEBaWlqZ5WlpacjMzNRKUUSPm9yrBXq0dERBsRJjfj2JRHYwJiKiCjwx3CxevBjR0dGq74cOHYo1a9aUWW/dunUYOnSodqsjekhPLsN3Q/3QzNEcSVkFGPvrSeQX6f6cSkREVPueGG5at26NgQMHYseOHQCAY8eOoXv37mXWCw0NxbFjx7RfIdFDFsYG+GVER9iYGuDsnQx8tOFchRNKEhFRw/XEcBMcHIx//vkHs2bNAgAUFBSguLjsPX+KioqQl5en/QqJHuFuZ4ofhvlDXy7D1rP38MP+G1KXREREOqZKfW48PT2xf/9+AECnTp3w008/lVln0aJF8Pf312pxROUJ8rbDjP5tAABf/R2DXRcTJK6IiIh0SZVn4zMwMAAAzJo1C2FhYTh79ix69uwJAIiMjMSJEyewa9eumqmS6DHDAj1wNSELK6JuYeKaM1j7Zme0a2wtdVlERKQDNB4t1bVrV0RFRcHNzQ3r1q3Dn3/+iaZNm+LcuXMIDg6uiRqJyvXZc60R3MweeUUKvL78JOLTcqUuiYiIdIBGN86sL3jjzPojK78ILy2KwpWELHg7mOGPcV1gbWoodVlERFQDtHbjzJSUFLXvT506hfPnz6u+37JlCwYMGIBPP/0UhYWFT1EykeYsjA2wfFQnuFgZ40ZyDsaujEZBMYeIExE1ZE8MNz/88AOmT5+u+v7NN9/E1atXAQA3b97EkCFDYGpqivXr1+Ojjz6qsUKJKuJsZYxlozqW3IMqNg0frDsLpbLBXZAkIqKHnhhuxo8fj5MnT+KNN94AAFy9ehV+fn4AgPXr1yMkJAS//fYbli9fjj/++KNGiyWqSEtnSyx6rWSI+LZz9zHnb95kk4iooXpiuLGzs8O2bdvQqlUrAIAQAkqlEgCwZ88e9O3bFwDg5uZWpgmLqDZ1bWqPOS+2AwAsPnATK47ESVsQERFJosqjpSZPngwACAgIwKxZs7By5UocOHAA/fr1AwDExsbCycmpZqokqqIX/Rvjg2ebAwCmbb2Izad5F3EiooZG46Hg8+bNw6lTpzBhwgT897//RdOmTQEAGzZsQJcuXbReIJGmJvRoihFBHgCAD9afReTlRIkrIiKi2qS1oeD5+fnQ09NTTfanyzgUvP5TKgUmrTuDzWfuwUhfjl9f74TAJnZSl0VERE+hqp/fVZ6h+HHR0dG4fPkygJKba3bo0KG6myLSOrlchq9e8kVWfjEiryRh9IqT+H1sZ/i4WkldGhER1TCNm6WSkpLQvXt3dOzYEe+++y7effddBAQEoGfPnkhOTq6JGomqxUBPjoXDOqCTly2yCooxYulx3EjOlrosIiKqYRqHm3feeQfZ2dm4ePEi0tLSkJaWhgsXLiAzMxPvvvtuTdRIVG3GBnr4eUQAfFwtkZpTiNd+PoY7D3ibBiKi+kzjPjdWVlbYs2cPOnbsqLb8+PHj6NWrF9LT07VZX41gn5uGJzW7AC8tjsLN5By42Zpg7dggNLI2kbosIiLSgNZuv/A4pVJZbqdhAwMD1fw3RLrGztwIq0cHwt3WFPFpeXh5yVEkZORLXRYREdUAjcNNjx49MHHiRNy7d0+17O7du3j//ffRs2dPrRZHpE0uVib4fWxnuNma4FZqLl5echRJmQw4RET1jcbhZsGCBcjMzISnpye8vb3h7e0NLy8vZGZm4vvvv6+JGom0xtXaBL+P6QxXaxPEpuSUBJwsBhwiovqkWvPcCCGwZ88eXLlScv+eVq1aISwsTOvF1RT2uaHbqbkY+lMU7mXko5mjOX4f2xn25kZSl0VERJWo6ue3xuHm119/xZAhQ2BkpP5BUFhYiDVr1mD48OHVq7gWMdwQAMSl5GDoT0eRkJmP5k7mWD26MxwsGHCIiHRVjYUbPT093L9/H46OjmrLU1NT4ejoCIVCUb2KaxHDDZW6mZyNoT8dRVJWAZrYm2H1mEC4WHEUFRGRLqqx0VJCCMhksjLL79y5Aysrzv5KdUsTB3OsezMIrtYmuJmSg8GLoxCfxnlwiIjqsirffqF9+/aQyWSQyWTo2bMn9PX/falCoUBsbCx69+5dI0US1SRPezOsfbMzhv18DLdSczF4cRRWjw5EEwdzqUsjIqJqqHK4GTBgAADgzJkzCA8Ph7n5v7/4DQ0N4enpiRdffFHrBRLVhsY2plj3ZhBeWXIUN5JzMHjxUaweHYgWzhZSl0ZERBrSqM+NQqHAqlWr0KtXL7i4uNRkXTWKfW6oIinZBXjtl+O4fD8TNqYG+PX1QLRtzOZWIiJdUCN9bvT09PDmm28iP5/zglD9ZG9uhN/HBMK3sRUe5Bbh5SVHceR6itRlERGRBjTuUOzj44ObN2/WRC1EOsHa1BCrRgciqIkdsguKMXLZCWw/f1/qsoiIqIo0DjezZs3C5MmTsW3bNty/fx+ZmZlqD6L6wMLYAMtGdUQfH2cUKpQY/9sprDx6S+qyiIioCjQON3379sXZs2fxwgsvoHHjxrCxsYGNjQ2sra1hY2NTrSIWLlwIT09PGBsbIzAwEMePH69w3Y0bNyIgIADW1tYwMzODn58fVq5cWa39ElXG2EAPC17pgGGB7hAC+GzzBXy7+yqqMak3ERHVoiqPliq1b98+rRawdu1aTJo0CYsWLUJgYCDmzZuH8PBwxMTElJkoEABsbW3x3//+Fy1btoShoSG2bduGUaNGwdHREeHh4VqtjUhPLsOsAT5wsDDCvD3X8F3kNaRkF2Bmfx/oycvO90RERNKr1r2ltCkwMBAdO3bEggULAABKpRJubm5455138Mknn1RpGx06dEC/fv3w+eefV2l9jpai6lh19BY+23IBQgA9Wzpi/svtYWak8d8HRERUTTU2Q3Gp3NxcXLlyBefOnVN7aKKwsBDR0dFqN92Uy+UICwtDVFTUE18vhEBkZCRiYmLQrVu3CtcrKChg3yB6aq929sCPwzrASF+OyCtJGLw4ComZHDlIRKRrNP6zMzk5GaNGjcKOHTvKfV6Te0ulpKRAoVDAyclJbbmTk5PqjuPlycjIgKurKwoKCqCnp4cffvgBzz77bIXrR0REYMaMGVWui6givX1c8PtYY4xZcRIX72ViwMLDWDqyI1q58AogEZGu0PjKzXvvvYf09HQcO3YMJiYm2LlzJ1asWIFmzZph69atNVFjGRYWFjhz5gxOnDiBL774ApMmTcL+/fsrXH/KlCnIyMhQPeLj42ulTqqfOrjbYNPbXeHtYIb7Gfl4aVEUDlxNlrosIiJ6SOMrN3v37sWWLVsQEBAAuVwODw8PPPvss7C0tERERAT69etX5W3Z29tDT08PiYmJassTExPh7Oxc4evkcjmaNm0KAPDz88Ply5cRERGB0NDQctc3MjKCkZFRlesiehJ3O1NsHNcVb646iaM30/D68hOY8UIbvNrZQ+rSiIgaPI2v3OTk5KhGMdnY2CA5ueQv1rZt2+LUqVMabcvQ0BD+/v6IjIxULVMqlYiMjERQUFCVt6NUKlFQUKDRvomeltXD2zMM6uAKhVLg/zZfwGebL6BIoZS6NCKiBk3jcNOiRQvExMQAAHx9fbF48WLcvXsXixYtqtb9piZNmoQlS5ZgxYoVuHz5MsaNG4ecnByMGjUKADB8+HBMmTJFtX5ERAR2796Nmzdv4vLly/j666+xcuVKvPrqqxrvm+hpGerL8fVLvvgwvAVkMmDl0Vt49edjSM1m2CYikorGzVITJ07E/fslU9FPmzYNvXv3xurVq2FoaIjly5drXMCQIUOQnJyMqVOnIiEhAX5+fti5c6eqk/Ht27chl/+bwXJycvD222/jzp07MDExQcuWLbFq1SoMGTJE430TaYNMJsP47k3RwskC7609g2OxaXhhwWH8NNwfbRrxpptERLWtyvPcxMbGwsvLq8zy0iHh7u7usLe313qBNYHz3FBNuZ6UhdErTiIuNRcmBnr430u+6NdO8yuaRERUVlU/v6scbko7D3fv3h09evRAaGgoGjdurLWCaxPDDdWkjNwiTPj9FA5eK7mb+ITuTTHp2eaQc0ZjIqKnovVJ/Pbu3YsRI0bg5s2bGDNmDDw8PNCsWTO8+eabWLNmTZkRT0QNlZWpAZaN7Iix3ZoAABbsu443VpxAem6hxJURETUM1br9Qn5+Po4cOYL9+/dj//79OH78OIqKitCyZUtcvHixJurUKl65odqy6fQdfPLHeRQUK+FqbYIfhnWAr5u11GUREdVJWm+WKk9hYSEOHz6MHTt2YPHixcjOztZohmKpMNxQbbp4LwNvrz6FW6m5MNCTYepzrfFqZw/IZGymIiLSRI3cW6qwsBD//PMPZsyYge7du8Pa2hpvvfUWHjx4gAULFiA2NvapCyeqb9o0ssKf7zyD3m2cUaQQ+GzLRby75gxyCoqlLo2IqF6q8pWbHj164NixY/Dy8kJISAiCg4MREhJSrbltpMYrNyQFIQR+ORSL2TuuoFgp4O1ghkWv+qOZk4XUpRER1Qlav3Jz8OBB2NnZoUePHujZsyeeffbZOhlsiKQik8kwOrgJ1oztDCdLI9xIzsELCw5j/cl4PEXrMBERPabK4SY9PR0//fQTTE1NMWfOHDRq1Aht27bFhAkTsGHDBtVtGIiocgGetvjr3WA809QeeUUKfLjhHN5dcwYZeUVSl0ZEVC9Uu0NxVlYWDh06hH379mH//v04e/YsmjVrhgsXLmi7Rq1jsxTpAoVSYNGBG/hm91UolAKu1iaY/7If/D1spS6NiEgn1UiH4keZmZnB1tYWtra2sLGxgb6+Pi5fvlzdzRE1OHrykts2bHgrCO62pribnofBi4/iuz3XoFCymYqIqLqqfOVGqVTi5MmT2L9/P/bt24fDhw8jJycHrq6u6N69u+rh4eFR0zU/NV65IV2TlV+EqVsuYtPpuwCATp62+HaoH1ytTSSujIhId2h9nhtLS0vk5OTA2dlZFWRCQ0Ph7e2ttaJrC8MN6apNp+/gs80XkV1QDEtjfczs74P+fo04Jw4REWog3CxevBjdu3dH8+bNtVakVBhuSJfdSs3BxDVncCY+HQDQu40zZg30gb25kbSFERFJrFZmKK6rGG5I1xUrlFh04Aa+i7yGIoWAnZkhvhjog94+nH6BiBquGu9QTEQ1R19Pjgk9mmHz+K5o6WyB1JxCvLXqFN5bcxoZuRwyTkRUGYYbIh3WppEVtkzoivHdvSGXAZvP3EOveQewLyZJ6tKIiHQWww2RjjPS18OH4S3xx7guaGJvhsTMAoxadgLvrz2DtJxCqcsjItI5DDdEdUR7dxv89W4w3njGCzIZsOn0XYR9cwBbztzl7RuIiB7BcENUh5gY6uGz51pj47guaOlsgbScQkxccwajlp/AnQe5UpdHRKQTGG6I6qD27jbYOuEZTO7VHIZ6cuyPSUavb//BssOxnN2YiBo8hhuiOspQv2RE1faJwejkaYvcQgVm/HkJg348gvN3MqQuj4hIMgw3RHVcU0dzrBnbGV8M9IGFkT7OxqfjhYWHMHXLBd5pnIgaJIYbonpALpdhWKAHIj8IQX+/RhAC+DXqFnp+vR9/RN9hh2MialA4QzFnKKZ66MiNFEzdchHXk7IBlNyI8/MBPmjhbCFxZURE1cfbL1SC4YYagsJiJX45FIv5kdeQV6SAnlyG4UEeeK9nc1iZGkhdHhGRxhhuKsFwQw3J3fQ8fP7nJey8mAAAsDY1wKRnm+OVTu7Q12PLNBHVHQw3lWC4oYbo4LVkfL7tEq4mljRVNXM0x2fPtUa35g4SV0ZEVDUMN5VguKGGqlihxO8n4vHNrhg8eHgDzh4tHfHffq3g7WAucXVERJVjuKkEww01dBm5RZi/9xpWHIlDsVJAXy7Da0EemNizGaxNDaUuj4ioXAw3lWC4ISpxMzkbX/x1GZFXSu4ybmGsj3Gh3hjVxQsmhnoSV0dEpI7hphIMN0Tq/rmajC+3X8aVhCwAgJOlEd4La46X/Buz0zER6QyGm0ow3BCVpVAKbDlzF1/vuoq76XkAgCYOZvgovAXC2zhDJpNJXCERNXQMN5VguCGqWEGxAquP3sb3e6+pOh37uVnjkz4t0bmJncTVEVFDxnBTCYYboifLyi/Ckn9uYsnBWOQVKQAAwc3s8V5Yc/h72EhcHRE1RAw3lWC4Iaq6pKx8zI+8hjXH41GsLPl1EdLcAe8/2xx+btbSFkdEDQrDTSUYbog0F5+WiwV7r2PDqTtQPAw53VuUhJx2ja2lLY6IGgSGm0ow3BBV3+3UXHy/9xo2nr6rCjlhrRzxXlhz+LhaSVwdEdVnDDeVYLghenpxKTmYv/caNp++i4cZBz1aOmJ8d2/4e9hKWxwR1UsMN5VguCHSnpvJ2ZgfeQ1bz95ThZxAL1uM794Uwc3sOYSciLSG4aYSDDdE2heXkoPF/9zAhug7KFKU/Fpp62qF8d290au1M+RyhhwiejoMN5VguCGqOfcz8vDzwVj8duy2agi5t4MZxoU2xQu+jWCozxmPiah6GG4qwXBDVPPScgqx/HAslh+JQ2Z+MYCS2zqM6OKJYZ08YGVqIHGFRFTXMNxUguGGqPZk5Rdh9bHbWHooFklZBQAAEwM9DA5ojNef8YKHnZnEFRJRXcFwUwmGG6LaV1isxJ9n72HJwZuqG3TKZEB4a2eM6ebFEVZE9EQMN5VguCGSjhACh6+n4udDN7E/Jlm1vL27NUY/0wS92jjBgHciJ6JyMNxUguGGSDdcTczCLwdjsenMXRQWKwEAzpbGGBbojqGd3OFgYSRxhUSkSxhuKsFwQ6RbkrMKsPLoLfx27BZSsgsBAAZ6MvRt64LhQR7o4G7D+XKIqMqf3zpx7XfhwoXw9PSEsbExAgMDcfz48QrXXbJkCYKDg2FjYwMbGxuEhYVVuj4R6T4HCyNMerY5Dn/SA98N9UMHd2sUKQS2nLmHF3+MwnPfH8LaE7eRV6iQulQiqgMkv3Kzdu1aDB8+HIsWLUJgYCDmzZuH9evXIyYmBo6OjmXWHzZsGLp27YouXbrA2NgYc+bMwaZNm3Dx4kW4urpWaZ+8ckOk+y7czcCvUXHYcuYeCh42WVmZGGBwQGO83MkdTRzMJa6QiGpbnWmWCgwMRMeOHbFgwQIAgFKphJubG9555x188sknT3y9QqGAjY0NFixYgOHDh1dpnww3RHXHg5xCrDsZj1XHbiE+LU+1PNDLFi93ckdvH2cYG+hJWCER1Zaqfn7r12JNZRQWFiI6OhpTpkxRLZPL5QgLC0NUVFSVtpGbm4uioiLY2lY8jLSgoAAFBQWq7zMzM6tfNBHVKhszQ7wZ4o3RwU2wPyYJq4/dxv6YJByLTcOx2DRYbTXAwPaueLmTO1o4W0hdLhHpAEnDTUpKChQKBZycnNSWOzk54cqVK1Xaxscff4xGjRohLCyswnUiIiIwY8aMp6qViKSlJ5ehZysn9GzlhPsZeVh34g7WnYzH3fQ8LD8Sh+VH4tDe3Rovd3THc74uMDWU9NcbEUlIJzoUV9fs2bOxZs0abNq0CcbGxhWuN2XKFGRkZKge8fHxtVglEWmbi5UJJoY1wz8fdcfyUR3Ru40z9OUynL6djo/+OIdOX0Ti4w3ncOxmKhrggFCiBk/SP23s7e2hp6eHxMREteWJiYlwdnau9LX/+9//MHv2bOzZswft2rWrdF0jIyMYGXG+DKL6Rk8uQ2gLR4S2cERSVj42RN/B2hPxuJWai7Un47H2ZDzcbE0wqH1jvNihMdztTKUumYhqgaRXbgwNDeHv74/IyEjVMqVSicjISAQFBVX4urlz5+Lzzz/Hzp07ERAQUBulEpGOc7QwxtuhTbHvg1CsHdsZgwMaw9xIH/Fpefgu8hq6fbUPLy06gjXHbyMzv0jqcomoBkk+Wmrt2rUYMWIEFi9ejE6dOmHevHlYt24drly5AicnJwwfPhyurq6IiIgAAMyZMwdTp07Fb7/9hq5du6q2Y25uDnPzqg0N5WgpooYhr1CBvy8m4I9Td3DoegpKf9sZ6csR3sYZgzq44pmm9tDn7R6I6oQ6MxQcABYsWICvvvoKCQkJ8PPzw/z58xEYGAgACA0NhaenJ5YvXw4A8PT0xK1bt8psY9q0aZg+fXqV9sdwQ9Tw3M/Iw6bTd/FH9B3cSM5RLbc3N0Tfti54wbcROrjbQC7nTMhEuqpOhZvaxnBD1HAJIXDuTgb+OHUHf569hwe5/zZRuVqb4DlfFzzfrhHaNLLkLR+IdAzDTSUYbogIAIoUShy6loI/z97D3xcTkPPI7R2aOJjhBd9GeMG3EWdDJtIRDDeVYLghosflFymw90oStp65h70xSaq7lANAm0aW6NfOBX18XOBlbyZhlUQNG8NNJRhuiKgyWflF2HUxEVvP3sOh6ylQKP/9NdnS2QK9fZzRx8cFzZ3M2XRFVIsYbirBcENEVZWWU4gdF+5j54UEHLmRqhZ0mtibqYKOjyv76BDVNIabSjDcEFF1pOcWYvelROy8kICD11JQqPi36aqxjQl6t3FGn7bOaO/GUVdENYHhphIMN0T0tLLyi7D3ShJ2XkjA/phk5BX92xnZwcIIPVs6IqyVE7o2tYeJIe9aTqQNDDeVYLghIm3KK1TgwNUk7LiQgL2Xk5BVUKx6zthAjmea2pfc9LOlIxwtK74PHhFVjuGmEgw3RFRTCooVOHYzDZGXE7HnchLupuepPe/b2AphD+9u3srFgv10iDTAcFMJhhsiqg1CCFxJyMKeS4nYcyUJZ+PT1Z53tTZBz1aOCG3hgKAmbL4iehKGm0ow3BCRFJIy87H3ShL2XE7EoespyC/6t0Oyob4cgV62CGnugNAWDvB24DBzoscx3FSC4YaIpJZXqMDh6ynYG5OEAzHJZZqvXK1NENLCASHNHdDF2w4WxgYSVUqkOxhuKsFwQ0S6RAiBG8k52B+ThANXk3EsNk1thmR9uQwBnjYIae6I4Gb2aO1iyaHm1CAx3FSC4YaIdFleoQJHb6biwNVkHLiajNiUHLXnbUwN0MXbHl2a2uGZpvZwtzVlExY1CAw3lWC4IaK65FZqTknQiSm5qpP9yFBzoKQJ65mm9ujazB5dvO1gb24kUaVENYvhphIMN0RUVxUplDh3Jx2Hr6fi0PUUnL79AEUK9V/jLZ0t0LWpPZ5pao9OXrYwM9KXqFoi7WK4qQTDDRHVF7mFxTgem4bD11Nw6HoqLt/PVHteTy6Dj6sVOjexRWcvOwR42rBzMtVZDDeVYLghovoqNbsAR26k4siNFBy6noL4NPVRWHIZ0KaRFQK9bBHYxA6dPG1hZcqwQ3UDw00lGG6IqKG48yAXx26m4VhsKo7FpuFWaq7a8zIZ0MrZEoFNbBHoZYdAL1vYmBlKVC1R5RhuKsFwQ0QN1f2MPByPTcPRm6k4djMNNx8biQUATR3NEeBhA/+HDy97M47GIp3AcFMJhhsiohJJmfk4Fvvwys7NNFxLyi6zjp2ZITp42CDAwwYBnjbwcbWCkT5vFUG1j+GmEgw3RETlS8spRPStBzh5Kw3RcQ9w7m6G2oSCQMmtItq5WsHf0wYBHrbw97CBLZuyqBYw3FSC4YaIqGoKihW4cDcT0bfScDLuAaJvPUBqTmGZ9bzszeDnZg0/N2v4ulmjlYsFr+6Q1jHcVILhhoioeoQQiEvNxcm4NETfKgk75TVlGerJ0bqRpSrw+LlZw8OOMynT02G4qQTDDRGR9qTnFuJMfLrqcTY+HQ9yi8qsZ21qAN/G1mpXeNicRZpguKkEww0RUc0RQuB2Wi7OxKfj9O10nL2Tjov3Msv03QEADztTtHW1Uj3aNLLivDtUIYabSjDcEBHVrsJiJS7fz8TZO+k4c7vkCk95w9ABwN22JPD4PAw8Pq6WsDblFR5iuKkUww0RkfQycotw9k46zt/NwIW7GTh/NwN3HuSVu25jGxO1wNPW1YqTDTZADDeVYLghItJND3IKcfFeplrguZ2WW+66rtYmaOViidYuFiX/NrKEm40p5HJ2Wq6vGG4qwXBDRFR3ZOQW4eK9kqBTGnriUssPPGaGemjpYolWpYHHxRItnC1gasg7o9cHDDeVYLghIqrbMvOLcOleJi7fzyz5NyETVxOzy+20LJMBXnZmaPUw9LRuZIlWLpZwtjTm0PQ6huGmEgw3RET1T5FCidiUnH9Dz/2Sf1Oyy046CJQMTW/uZIHmTuZo4WTx8GsL9uXRYQw3lWC4ISJqOJKy8nH5fhYu3//3Ss/NlBwolOV//DlYGKnCTgtnczR7+LW5EZu2pMZwUwmGGyKihi2/SIHrSdm4mpiFmMQsXEvMRkxCFu6mlz9aCyjpwNzC2QLNHrnS09TRHMYGvM1EbWG4qQTDDRERlSe7oBjXErNKQk9CNq4lZSEmIQtJWQXlri+TlQxTb+pgDm8HczR1NIe3Y8nXnH1Z+xhuKsFwQ0REmkjPLcTVxGzEJGbhakKW6opPejm3mShla2YIbwezksDjUBJ6mjqYw9XahMPVq4nhphIMN0RE9LSEEEjNKcSNpGxcT87GjaSch/9mV9q8ZaQvRxMHc3g7mP17tcfBHJ72phyy/gRV/fzmT5GIiKgaZDIZ7M2NYG9uhMAmdmrP5RYW42ZyDm48DDs3knNwPSkbsSk5KHh4K4rL9zPLbNPJ0ghe9mbwsjeDp52Z6mt3O1MY6bNvT1Xxyg2v3BARUS1RKAXuPMjF9aTsh8Gn5GrPzeTscu+kXkomAxpZmaCJQ0no8bQ3QxP7kn8b25jAQE9ei0chHTZLVYLhhoiIdE16biFiU3IQl5qD2JTckq8fPrIKiit8nb5chsY2JiVXe0qv9Niawt3WFI1tTGGoX3+CD8NNJRhuiIiorhBCICW78GHoKQk7sQ8fcak5yC8qOytzqdIrPm62JvCwLWnecrM1hcfD8GNtalCnZmlmuKkEww0REdUHSqVAYlb+w9CTqwpA8Wm5uJWai7wiRaWvtzDSh7udqepKT+nXHrZmcLE21rnmLoabSjDcEBFRfVd6xed2Wi5up+Xgdmrev1+n5SIxs/y5e0rpyWVoZG2s1sTV2Mbk4cMUDuZGtT6kneGmEgw3RETU0OUXKRCflovbD6/y3E7LLbni8/DfgnJuQvooQ305GlubwPVh2CkNPm62JV87mBtpvcmLQ8GJiIioQsYGemjmZIFmThZlnlMqBZKzC1Sh586DXNx5kIf4tJJ/72fkobBYiZspObiZklPu9he+0gH92rnU9GGUi+GGiIiI1MjlMjhZGsPJ0hidvGzLPF+kUCIhIx93HuThzoNcxD/8986DPNx9GH4a25hIUHkJhhsiIiLSiIGeHG62JSOvALsyzxcplJBLOAqL4YaIiIi0SupRVjoxxmvhwoXw9PSEsbExAgMDcfz48QrXvXjxIl588UV4enpCJpNh3rx5tVcoERER6TzJw83atWsxadIkTJs2DadOnYKvry/Cw8ORlJRU7vq5ublo0qQJZs+eDWdn51quloiIiHSd5OHmm2++wZgxYzBq1Ci0bt0aixYtgqmpKZYuXVru+h07dsRXX32FoUOHwsjIqJarJSIiIl0nabgpLCxEdHQ0wsLCVMvkcjnCwsIQFRWltf0UFBQgMzNT7UFERET1k6ThJiUlBQqFAk5OTmrLnZyckJCQoLX9REREwMrKSvVwc3PT2raJiIhIt0jeLFUbpkyZgoyMDNUjPj5e6pKIiIiohkg6FNze3h56enpITExUW56YmKjVzsJGRkbsn0NERNRASHrlxtDQEP7+/oiMjFQtUyqViIyMRFBQkISVERERUV0l+SR+kyZNwogRIxAQEIBOnTph3rx5yMnJwahRowAAw4cPh6urKyIiIgCUdEK+dOmS6uu7d+/izJkzMDc3R9OmTSU7DiIiItINkoebIUOGIDk5GVOnTkVCQgL8/Pywc+dOVSfj27dvQy7/9wLTvXv30L59e9X3//vf//C///0PISEh2L9/f22XT0RERDpGJoQQUhdR26p6y3QiIiLSHVX9/G4Qo6WIiIio4WC4ISIionpF8j43UihtieNMxURERHVH6ef2k3rUNMhwk5WVBQCcqZiIiKgOysrKgpWVVYXPN8gOxUqlEvfu3YOFhQVkMpnWtpuZmQk3NzfEx8fX247K9f0Y6/vxAfX/GOv78QH1/xjr+/EB9f8Ya+r4hBDIyspCo0aN1EZSP65BXrmRy+Vo3LhxjW3f0tKyXr5ZH1Xfj7G+Hx9Q/4+xvh8fUP+Psb4fH1D/j7Emjq+yKzal2KGYiIiI6hWGGyIiIqpXGG60yMjICNOmTavXN+ms78dY348PqP/HWN+PD6j/x1jfjw+o/8co9fE1yA7FREREVH/xyg0RERHVKww3REREVK8w3BAREVG9wnBDRERE9QrDjRYtXLgQnp6eMDY2RmBgII4fPy51SdUyffp0yGQytUfLli1Vz+fn52P8+PGws7ODubk5XnzxRSQmJkpY8ZP9888/eP7559GoUSPIZDJs3rxZ7XkhBKZOnQoXFxeYmJggLCwM165dU1snLS0Nw4YNg6WlJaytrfHGG28gOzu7Fo+iYk86vpEjR5Y5p71791ZbR5ePLyIiAh07doSFhQUcHR0xYMAAxMTEqK1Tlffl7du30a9fP5iamsLR0REffvghiouLa/NQKlSVYwwNDS1zHt966y21dXT1GH/88Ue0a9dONalbUFAQduzYoXq+rp8/4MnHWJfPX3lmz54NmUyG9957T7VMZ86jIK1Ys2aNMDQ0FEuXLhUXL14UY8aMEdbW1iIxMVHq0jQ2bdo00aZNG3H//n3VIzk5WfX8W2+9Jdzc3ERkZKQ4efKk6Ny5s+jSpYuEFT/Z9u3bxX//+1+xceNGAUBs2rRJ7fnZs2cLKysrsXnzZnH27FnxwgsvCC8vL5GXl6dap3fv3sLX11ccPXpUHDx4UDRt2lS8/PLLtXwk5XvS8Y0YMUL07t1b7ZympaWpraPLxxceHi6WLVsmLly4IM6cOSP69u0r3N3dRXZ2tmqdJ70vi4uLhY+PjwgLCxOnT58W27dvF/b29mLKlClSHFIZVTnGkJAQMWbMGLXzmJGRoXpel49x69at4q+//hJXr14VMTEx4tNPPxUGBgbiwoULQoi6f/6EePIx1uXz97jjx48LT09P0a5dOzFx4kTVcl05jww3WtKpUycxfvx41fcKhUI0atRIRERESFhV9UybNk34+vqW+1x6erowMDAQ69evVy27fPmyACCioqJqqcKn8/iHv1KpFM7OzuKrr75SLUtPTxdGRkbi999/F0IIcenSJQFAnDhxQrXOjh07hEwmE3fv3q212quionDTv3//Cl9Tl45PCCGSkpIEAHHgwAEhRNXel9u3bxdyuVwkJCSo1vnxxx+FpaWlKCgoqN0DqILHj1GIkg/HRz9IHlfXjtHGxkb8/PPP9fL8lSo9RiHqz/nLysoSzZo1E7t371Y7Jl06j2yW0oLCwkJER0cjLCxMtUwulyMsLAxRUVESVlZ9165dQ6NGjdCkSRMMGzYMt2/fBgBER0ejqKhI7VhbtmwJd3f3OnussbGxSEhIUDsmKysrBAYGqo4pKioK1tbWCAgIUK0TFhYGuVyOY8eO1XrN1bF//344OjqiRYsWGDduHFJTU1XP1bXjy8jIAADY2toCqNr7MioqCm3btoWTk5NqnfDwcGRmZuLixYu1WH3VPH6MpVavXg17e3v4+PhgypQpyM3NVT1XV45RoVBgzZo1yMnJQVBQUL08f48fY6n6cP7Gjx+Pfv36qZ0vQLf+HzbIG2dqW0pKChQKhdrJAgAnJydcuXJFoqqqLzAwEMuXL0eLFi1w//59zJgxA8HBwbhw4QISEhJgaGgIa2trtdc4OTkhISFBmoKfUmnd5Z2/0ucSEhLg6Oio9ry+vj5sbW3rxHH37t0bgwYNgpeXF27cuIFPP/0Uffr0QVRUFPT09OrU8SmVSrz33nvo2rUrfHx8AKBK78uEhIRyz3Hpc7qkvGMEgFdeeQUeHh5o1KgRzp07h48//hgxMTHYuHEjAN0/xvPnzyMoKAj5+fkwNzfHpk2b0Lp1a5w5c6benL+KjhGo++cPANasWYNTp07hxIkTZZ7Tpf+HDDdURp8+fVRft2vXDoGBgfDw8MC6detgYmIiYWVUXUOHDlV93bZtW7Rr1w7e3t7Yv38/evbsKWFlmhs/fjwuXLiAQ4cOSV1KjanoGMeOHav6um3btnBxcUHPnj1x48YNeHt713aZGmvRogXOnDmDjIwMbNiwASNGjMCBAwekLkurKjrG1q1b1/nzFx8fj4kTJ2L37t0wNjaWupxKsVlKC+zt7aGnp1emR3hiYiKcnZ0lqkp7rK2t0bx5c1y/fh3Ozs4oLCxEenq62jp1+VhL667s/Dk7OyMpKUnt+eLiYqSlpdXJ427SpAns7e1x/fp1AHXn+CZMmIBt27Zh3759aNy4sWp5Vd6Xzs7O5Z7j0ud0RUXHWJ7AwEAAUDuPunyMhoaGaNq0Kfz9/REREQFfX19899139er8VXSM5alr5y86OhpJSUno0KED9PX1oa+vjwMHDmD+/PnQ19eHk5OTzpxHhhstMDQ0hL+/PyIjI1XLlEolIiMj1dpa66rs7GzcuHEDLi4u8Pf3h4GBgdqxxsTE4Pbt23X2WL28vODs7Kx2TJmZmTh27JjqmIKCgpCeno7o6GjVOnv37oVSqVT9gqpL7ty5g9TUVLi4uADQ/eMTQmDChAnYtGkT9u7dCy8vL7Xnq/K+DAoKwvnz59VC3O7du2FpaalqNpDSk46xPGfOnAEAtfOoy8f4OKVSiYKCgnpx/ipSeozlqWvnr2fPnjh//jzOnDmjegQEBGDYsGGqr3XmPGqta3IDt2bNGmFkZCSWL18uLl26JMaOHSusra3VeoTXFR988IHYv3+/iI2NFYcPHxZhYWHC3t5eJCUlCSFKhvq5u7uLvXv3ipMnT4qgoCARFBQkcdWVy8rKEqdPnxanT58WAMQ333wjTp8+LW7duiWEKBkKbm1tLbZs2SLOnTsn+vfvX+5Q8Pbt24tjx46JQ4cOiWbNmunMUOnKji8rK0tMnjxZREVFidjYWLFnzx7RoUMH0axZM5Gfn6/ahi4f37hx44SVlZXYv3+/2jDa3Nxc1TpPel+WDkHt1auXOHPmjNi5c6dwcHDQmWG2TzrG69evi5kzZ4qTJ0+K2NhYsWXLFtGkSRPRrVs31TZ0+Rg/+eQTceDAAREbGyvOnTsnPvnkEyGTycSuXbuEEHX//AlR+THW9fNXkcdHgOnKeWS40aLvv/9euLu7C0NDQ9GpUydx9OhRqUuqliFDhggXFxdhaGgoXF1dxZAhQ8T169dVz+fl5Ym3335b2NjYCFNTUzFw4EBx//59CSt+sn379gkAZR4jRowQQpQMB//ss8+Ek5OTMDIyEj179hQxMTFq20hNTRUvv/yyMDc3F5aWlmLUqFEiKytLgqMpq7Ljy83NFb169RIODg7CwMBAeHh4iDFjxpQJ3rp8fOUdGwCxbNky1TpVeV/GxcWJPn36CBMTE2Fvby8++OADUVRUVMtHU74nHePt27dFt27dhK2trTAyMhJNmzYVH374odo8KULo7jG+/vrrwsPDQxgaGgoHBwfRs2dPVbARou6fPyEqP8a6fv4q8ni40ZXzKBNCCO1dByIiIiKSFvvcEBERUb3CcENERET1CsMNERER1SsMN0RERFSvMNwQERFRvcJwQ0RERPUKww0RERHVKww3RFRvTZ8+HX5+flKXUaHH6xs5ciQGDBggWT1E9QUn8SMiNSNHjsSKFSsQERGBTz75RLV88+bNGDhwIOrSr4zs7GwUFBTAzs4OQMmxpaenY/PmzdIW9tDj9WVkZEAIAWtra2kLI6rjeOWGiMowNjbGnDlz8ODBA6lLqZLCwsJyl5ubm6uCQ23sT1OP12dlZcVgQ6QFDDdEVEZYWBicnZ0RERFR4TrlNfnMmzcPnp6equ9Lm1m+/PJLODk5wdraGjNnzkRxcTE+/PBD2NraonHjxli2bJnaduLj4zF48GBYW1vD1tYW/fv3R1xcXJntfvHFF2jUqBFatGjxxBqnT5+OFStWYMuWLZDJZJDJZNi/f/9T7W/lypUICAiAhYUFnJ2d8corr6jd7RgALl68iOeeew6WlpawsLBAcHAwbty4Ue7PkM1SRNrBcENEZejp6eHLL7/E999/jzt37jzVtvbu3Yt79+7hn3/+wTfffINp06bhueeeg42NDY4dO4a33noLb775pmo/RUVFCA8Ph4WFBQ4ePIjDhw/D3NwcvXv3VrtiEhkZiZiYGOzevRvbtm17Yh2TJ0/G4MGD0bt3b9y/fx/3799Hly5dnmp/RUVF+Pzzz3H27Fls3rwZcXFxGDlypOo1d+/eRbdu3WBkZIS9e/ciOjoar7/+OoqLi5/qZ0pEldOXugAi0k0DBw6En58fpk2bhl9++aXa27G1tcX8+fMhl8vRokULzJ07F7m5ufj0008BAFOmTMHs2bNx6NAhDB06FGvXroVSqcTPP/8MmUwGAFi2bBmsra2xf/9+9OrVCwBgZmaGn3/+GYaGhlWqw9zcHCYmJigoKICzs7Nq+apVq6q9v9dff131dZMmTTB//nx07NgR2dnZMDc3x8KFC2FlZYU1a9bAwMAAANC8efPq/iiJqIp45YaIKjRnzhysWLECly9frvY22rRpA7n83181Tk5OaNu2rep7PT092NnZqZpzzp49i+vXr8PCwgLm5uYwNzeHra0t8vPzVc05ANC2bdsqB5vKPM3+oqOj8fzzz8Pd3R0WFhYICQkBANy+fRsAcObMGQQHB6uCDRHVDl65IaIKdevWDeHh4ZgyZYpacwsAyOXyMiOnioqKymzj8Q92mUxW7jKlUgmgZASRv78/Vq9eXWZbDg4Oqq/NzMw0OpaKVHd/OTk5CA8PR3h4OFavXg0HBwfcvn0b4eHhquYsExMTrdRIRJphuCGiSs2ePRt+fn5lOu06ODggISEBQghVc86ZM2eeen8dOnTA2rVr4ejoCEtLy6fe3qMMDQ2hUCi0sr8rV64gNTUVs2fPhpubGwDg5MmTauu0a9cOK1asQFFREa/eENUiNksRUaXatm2LYcOGYf78+WrLQ0NDkZycjLlz5+LGjRtYuHAhduzY8dT7GzZsGOzt7dG/f38cPHgQsbGx2L9/P959992n7tzs6emJc+fOISYmBikpKSgqKqr2/tzd3WFoaIjvv/8eN2/exNatW/H555+rrTNhwgRkZmZi6NChOHnyJK5du4aVK1ciJibmqY6DiCrHcENETzRz5kxVs1GpVq1a4YcffsDChQvh6+uL48ePY/LkyU+9L1NTU/zzzz9wd3fHoEGD0KpVK7zxxhvIz89/6is5Y8aMQYsWLRAQEAAHBwccPny42vtzcHDA8uXLsX79erRu3RqzZ8/G//73P7V17OzssHfvXmRnZyMkJAT+/v5YsmQJr+IQ1TDOUExEJJEpU6bg4MGDOHTokNSlENUrvHJDRFTLhBC4ceMGIiMj0aZNG6nLIap3GG6IiGpZRkYGWrduDUNDQ9V8P0SkPWyWIiIionqFV26IiIioXmG4ISIionqF4YaIiIjqFYYbIiIiqlcYboiIiKheYbghIiKieoXhhoiIiOoVhhsiIiKqVxhuiIiIqF75f7T7Px2Na6MUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Trening modelu (na danych treningowych)\n",
    "theta = train_logistic_regression(Xtrain_norm,ytrain)\n",
    "# Dokonanie predykcji (na danych testowych) i obliczenie dokładności modelu\n",
    "pred = predict_logistic_regression(Xtest_norm,theta)\n",
    "accuracy = np.mean(pred==ytest)\n",
    "print(\"Dokładność modelu na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli na wykresie obserwujesz stopniowy spadek kosztu - świetnie, Twój model się uczy! Można też zaobserwować bardzo wysoką wartość dokładności (**dokładność** to procent decyzji, które model podjął właściwie) naszego modelu na danych z zestawu testowego, świadczącą o jego poprawnym działaniu. \n",
    "\n",
    "\n",
    "## 4.2. Uruchomienie klasyfikatorów z biblioteki Scikit-learn\n",
    "\n",
    "### Import nowych niezbędnych klas\n",
    "\n",
    "Udało Ci się stworzyć model klasyfikatora opartego o regresję logistyczną od zera. Na szczęście nie zawsze trzeba włożyć tyle pracy, aby móc używać algorytmów uczenia maszynowego. Istnieją biblioteki posiadające gotowe implementacje wielu z nich. Jedną z takich bibliotek jest wykorzystywana już przez Ciebie biblioteka Scikit-learn. Tym razem do sprawdzenia, jak z analizowanym tutaj zbiorze danych iris radzą sobie inne algorytmy, wykorzystamy jej gotowe klasy i metody:\n",
    "- model regresji logistycznej - `linear_model.LogisticRegression` (dokumentacja [TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)), \n",
    "- model drzewa decyzyjnego - `tree.DecisionTreeClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),\n",
    "- model *k*-NN - `neighbors.KNeighborsClassifier` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)),\n",
    "- metrykę dokładności - `metrics.accuracy_score` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)),\n",
    "- klasę standaryzującą dane - `preprocessing.StandardScaler` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)),\n",
    "- funkcję dzielącą dane na zestaw treningowy i testowy - `model_selection.train_test_split` ([TUTAJ](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "Uruchom poniższą komórkę, aby zaimportować niezbędne klasy i metody. Zapoznaj się też z ich dokumentacją."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych z użyciem gotowych metod\n",
    "\n",
    "Mając wciąż w pamięci przechowywane zmienne X i y z oryginalnym, jeszcze nieznormalizowanym ani niepodzielonym zbiorem danych iris (oraz etykietami), możemy je ponownie wykorzystać w tej części ćwiczenia. Napisz więc fragment kodu, który:\n",
    "* dzieli dane X i y na właściwe zestawy Xtrain i Xtest (użyj metody `train_test_split`) - niech zestaw treningowy zawiera 70% danych,\n",
    "* standaryzuje Xtrain i Xtest (zwróć uwagę na metodę `fit_transform` klasy `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wymiary danych treningowych: (70, 4)\n",
      "Wymiary danych testowych: (30, 4)\n",
      "Przykładowe znormalizowane dane treningowe: \n",
      "[[-1.0280556   0.72748234 -0.92621165 -1.09332559]\n",
      " [-1.32853114  0.29036849 -1.06870575 -1.09332559]\n",
      " [ 0.77479765  0.72748234  1.1399528   1.42775459]\n",
      " [-0.42710452  0.72748234 -1.06870575 -1.09332559]\n",
      " [-0.72758006 -1.67664385  0.2849882   0.34729166]]\n"
     ]
    }
   ],
   "source": [
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Podziel dane z X i y na zestaw treningowy (70%) i testowy (30%) z wykorzystaniem metody train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# Dokonaj normalizacji danych treningowych z wykorzystaniem klasy StandardScaler - zwróć Xtrain_norm\n",
    "scaler = StandardScaler()\n",
    "Xtrain_norm = scaler.fit_transform(Xtrain)\n",
    "# Dokonaj normalizacji danych testowych - zwróć Xtest_norm\n",
    "Xtest_norm = scaler.transform(Xtest)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Wymiary danych treningowych: \" + str(Xtrain.shape))\n",
    "print(\"Wymiary danych testowych: \" + str(Xtest.shape))\n",
    "print(\"Przykładowe znormalizowane dane treningowe: \")\n",
    "print(Xtrain_norm[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresja logistyczna z biblioteki Scikit-learn\n",
    "\n",
    "Mamy przygotowane dane, pora zatem wytrenować model regresji logistycznej z wykorzystaniem klasy `LogisticRegression`!\n",
    "* Do trenowania (na bazie zestandaryzowanego zestawu treningowego) użyj metody `fit`.\n",
    "* Do dokonania predykcji użyj metody `predict` - zobacz, jakich predykcji dokona Twój model na widok danych z zestandaryzowanego zestawu testowego (zapisz je do zmiennej o nazwie `pred`).\n",
    "* Oblicz dokładność predykcji Twojego modelu (dla zestandaryzowanych danych testowych) z wykorzystaniem metody `accuracy_score`. Wynik zapisz do zmiennej o nazwie `acccuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność modelu na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression() # Leave the default settings\n",
    "\n",
    "# ------------ UZUPEŁNIJ KOD -------------\n",
    "# Wytrenuj model regresji logistycznej na zestandaryzowanych danych treningowych\n",
    "model.fit(Xtrain_norm, ytrain)\n",
    "\n",
    "# Zwróć predykcję modelu (do zmiennej o nazwie pred) dla zestandaryzowanych danych testowych\n",
    "pred = model.predict(Xtest_norm)\n",
    "\n",
    "# Oblicz dokładność predykcji\n",
    "accuracy = accuracy_score(ytest, pred)\n",
    "# ----------------------------------------\n",
    "\n",
    "print(\"Dokładność modelu na danych testowych: \" + str(accuracy * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inne klasyfikatory dostępne w Scikit-learn\n",
    "\n",
    "Na sam koniec, sprawdź, jak zachowują się inne modele klasyfikatorów, dostępne w ramach biblioteki Scikit-learn: drzewo decyzyjne oraz *k*-NN! Uruchom po prostu poniższy kod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokładność drzewa decyzyjnego na danych testowych: 100.0%\n",
      "Dokładność k-NN na danych testowych: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Drzewo decyzyjne\n",
    "model2 = DecisionTreeClassifier(max_depth=5)\n",
    "model2.fit(Xtrain_norm,ytrain)\n",
    "pred = model2.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność drzewa decyzyjnego na danych testowych: \"+str(accuracy*100)+'%')\n",
    "\n",
    "# k-NN\n",
    "model3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model3.fit(Xtrain_norm,ytrain)\n",
    "pred = model3.predict(Xtest_norm)\n",
    "accuracy = accuracy_score(ytest,pred)\n",
    "print(\"Dokładność k-NN na danych testowych: \"+str(accuracy*100)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gratulacje! W ten sposób zakończyliśmy to ćwiczenie, w którym skupiliśmy się na działaniu klasycznych algorytmów klasyfikacji.\n",
    "\n",
    "\n",
    "## 5. Pytania kontrolne\n",
    "\n",
    "1. Czym różni się uczenie nadzorowane od nienadzorowanego?\n",
    "2. Jakie są różnice pomiędzy problemem regresji a klasyfikacji?\n",
    "3. Opisz krótko, na czym polega trening modelu uczenia maszynowego z wykorzystaniem metody gradientów prostych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Odp\n",
    "\n",
    "1. Różnica między uczeniem nadzorowanym a nienadzorowanym polega na obecności lub braku etykiet wyjściowych w danych treningowych oraz na różnych celach, jakie te dwa rodzaje uczenia mają. W uczeniu nadzorowanym modele są trenowane do przewidywania lub klasyfikacji na podstawie dostarczonych etykiet, podczas gdy w uczeniu nienadzorowanym celem jest wyodrębnienie ukrytych wzorców lub struktur w danych.\n",
    "\n",
    "2. Różnica między problemem regresji a klasyfikacji polega na rodzaju wyników, które model próbuje przewidzieć. W regresji przewiduje się liczby rzeczywiste, podczas gdy w klasyfikacji przyporządkowuje się dane do dyskretnych kategorii lub klas.\n",
    "\n",
    "3. Trening modelu uczenia maszynowego przy użyciu metody gradientów prostych (Gradient Descent) jest jednym z najważniejszych procesów w uczeniu maszynowym, szczególnie w kontekście uczenia nadzorowanego. \n",
    "\n",
    "- Celem treningu jest dostosowanie modelu do danych treningowych w taki sposób, aby osiągnąć jak najlepsze dopasowanie modelu do rzeczywistych danych. \n",
    "- W procesie treningu określa się funkcję kosztu (ang. cost function), która mierzy, jak dobrze model przewiduje dane treningowe w porównaniu do rzeczywistych etykiet.\n",
    "- Gradient to wektor pochodnych cząstkowych funkcji kosztu względem parametrów modelu. Wskazuje kierunek, w którym funkcja kosztu rośnie najszybciej. Proces treningu polega na iteracyjnym aktualizowaniu parametrów modelu, aby zmniejszyć wartość funkcji kosztu. \n",
    "- Parametr zwany \"szybkością uczenia\" kontroluje, jak duże kroki są podejmowane podczas aktualizacji parametrów modelu. Zbyt duże kroki mogą prowadzić do rozbieżności, a zbyt małe kroki mogą spowodować wolny postęp. Optymalna wartość szybkości uczenia jest ważna.\n",
    "- Proces treningu polega na wielokrotnym obliczaniu gradientu, aktualizowaniu parametrów modelu i powtarzaniu tego procesu przez wiele iteracji. Ilość iteracji zależy od algorytmu i problemu, który rozwiązujemy.\n",
    "- Proces treningu kończy się, gdy osiągniemy satysfakcjonujący poziom zbieżności, czyli kiedy wartość funkcji kosztu jest już na niskim poziomie lub przestaje się znacząco zmieniać.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d1a1735bd163b32102b8ea4514749acd69bcad8d489552073ec2e1a14a559f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
